{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c123350",
   "metadata": {},
   "source": [
    "# PROGRES - TME2\n",
    "\n",
    "Fabien Mathieu - fabien.mathieu@normalesup.org\n",
    "\n",
    "Sébastien Tixeuil - Sebastien.Tixeuil@lip6.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bad02",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- Star exercises (indicated by *) should only be done if all other exercises have been completed. You \n",
    "don't have to do them if you do not want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3ddc2",
   "metadata": {},
   "source": [
    "# Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8b655",
   "metadata": {},
   "source": [
    "1. Cite your sources\n",
    "2. One file to rule them all\n",
    "3. Explain\n",
    "4. Execute your code\n",
    "\n",
    "\n",
    "https://github.com/balouf/progres/blob/main/rules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3dbfd",
   "metadata": {},
   "source": [
    "# Name of the Students\n",
    "\n",
    "Nesrine Naaman (DIGIT track) <br>\n",
    "Isaac Balam Rodriguez Arellano (DIGIT track)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb50c3c",
   "metadata": {},
   "source": [
    "# Exercice 1 - Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe417ea0",
   "metadata": {},
   "source": [
    "Consider the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c9d17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.166734Z",
     "start_time": "2024-10-11T07:03:31.162693Z"
    }
   },
   "outputs": [],
   "source": [
    "L = ['marie.Dupond@gmail.com', 'lucie.Durand@wanadoo.fr', \n",
    "'Sophie.Parmentier @@ gmail.com', 'franck.Dupres.gmail.comm', \n",
    "'pierre.Martin@lip6 .fr ',' eric.Deschamps@gmail.com '] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63105eb",
   "metadata": {},
   "source": [
    "- Which of these entries are valid?\n",
    "- Use regular expressions to identify valid *gmail* addresses and display them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cd70e",
   "metadata": {},
   "source": [
    "Among the entries provided, these are the valid email addresses: <br>\n",
    "marie.Dupond@gmail.com <br>\n",
    "lucie.Durand@wanadoo.fr <br>\n",
    " eric.Deschamps@gmail.com <br>\n",
    " \n",
    "the rest are in invalid formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6e8b2",
   "metadata": {},
   "source": [
    "To check if each of the entries are valid gmail addresses or not, we created a true_gmail function which takes a list of emails and returns an array of those that are in true gmail formats only. We looped through the mail list and for each string, we checked if it's a valid gmail address using the following regular expression (4) :\n",
    "\n",
    "\\s*[a-zA-Z0-9](\\.?[a-zA-Z0-9]){4,28}[a-zA-Z0-9]@gmail\\.com\\s*\n",
    "\n",
    "Explanation: (2)(3)\n",
    "\n",
    "* \\s* allows a whitespace 0 or more times at the beginning of the email (since we considered ' eric.Deschamps@gmail.com ' to be valid) <br>\n",
    "* [a-zA-Z0-9] because we want the first character to be a letter or a number <br>\n",
    "* (\\.?[a-zA-Z0-9]){4,28} we want to allow after that any character that's either a letter, a number, or a dot. also, the local part before the domain should be between 6 and 30 characters. so 4 and 28 because we deducted for the 1st character and the last character. <br>\n",
    "* [a-zA-Z0-9] because we want the last character before the domain part to be either a letter or a number <br>\n",
    "* @gmail\\.com we want to match @gmail.com exactly as is <br>\n",
    "* \\s* to match a 0 occurrence of whitespace or more <br>\n",
    "\n",
    "Sources: <br> https://docs.python.org/es/3/library/re.html (1) <br>\n",
    "https://docs.mapp.com/docs/email-address-validation (2) <br>\n",
    "https://accounts.google.com (3) <br>\n",
    "Giang le and Yalda Eftekhari (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcccc2",
   "metadata": {},
   "source": [
    "Applying the function we created on the L list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5366003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.182237Z",
     "start_time": "2024-10-11T07:03:31.167743Z"
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "def true_gmail(mail_list):\n",
    "    M = []\n",
    "    for i in range(len(mail_list)):\n",
    "        pattern = r'\\s*[a-zA-Z0-9](\\.?[a-zA-Z0-9]){4,28}[a-zA-Z0-9]@gmail\\.com\\s*'\n",
    "        m = re.fullmatch(pattern, mail_list[i])\n",
    "        if m:\n",
    "            M.append(mail_list[i])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6de182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.210273Z",
     "start_time": "2024-10-11T07:03:31.196497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid emails:  ['marie.Dupond@gmail.com', ' eric.Deschamps@gmail.com ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Valid emails: \", true_gmail(L))\n",
    "#Here we call the function and print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109b9f1",
   "metadata": {},
   "source": [
    "- Use regular expressions to check if a string ends with a number. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c1fe5",
   "metadata": {},
   "source": [
    "For this part, we created a function ends_with_number that takes a strings, returns true if the string ends with a number, false otherwise.\n",
    "\n",
    "To check for that, we used the following regular expression: '\\d$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c51df0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.221802Z",
     "start_time": "2024-10-11T07:03:31.211282Z"
    }
   },
   "outputs": [],
   "source": [
    "def ends_with_number(txt):\n",
    "    txt = re.search(r'\\d$', txt)\n",
    "    if txt:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# the \\d represents any number and $ represents the end of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ded133e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.246076Z",
     "start_time": "2024-10-11T07:03:31.235381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(ends_with_number('to42to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ce2c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.257782Z",
     "start_time": "2024-10-11T07:03:31.248084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ends_with_number('to42to666'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162343e8",
   "metadata": {},
   "source": [
    "- Use regular expressions to remove problematic zeros from an IPv4 address expressed as a \n",
    "string. (example: \"216.08.094.196\" should become \"216.8.94.196\", but \"216.80.140.196\" \n",
    "should remain \"216.80.140.196\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90a687",
   "metadata": {},
   "source": [
    "For this part, in the function normalize_ip, we used the regular expression: '(^|\\.)0+(?=[^.])', r'\\1' <br>\n",
    "the section (^|.) indicates that the 0 could be at the begining of the string or after a . the 0+ searches one or more zeros, this (?=[^.]) is the lookahead which means that we are looking for zeros that are not at the end of a subdomain and the r'\\1' will replace the zeros with the beggining of the string or with a . <br>\n",
    "Source: https://stackoverflow.com/questions/58013274/python-remove-leading-zeros-from-ip-addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704e5b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.271323Z",
     "start_time": "2024-10-11T07:03:31.261798Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_ip(txt):\n",
    "    aux = re.sub(r'(^|\\.)0+(?=[^.])', r'\\1', txt)\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "650d10e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.295246Z",
     "start_time": "2024-10-11T07:03:31.284633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.0.94.196\n"
     ]
    }
   ],
   "source": [
    "print(normalize_ip(\"216.0.094.196\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84bb75cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.309262Z",
     "start_time": "2024-10-11T07:03:31.296255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.8.94.196\n"
     ]
    }
   ],
   "source": [
    "print(normalize_ip(\"216.08.094.196\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39f7decb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.350525Z",
     "start_time": "2024-10-11T07:03:31.313271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.80.140.196\n"
     ]
    }
   ],
   "source": [
    "print(normalize_ip(\"216.80.140.196\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f0979daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.8.94.196\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(normalize_ip(\"0216.008.0094.196\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d981409",
   "metadata": {},
   "source": [
    "- Use regular expressions to transform a date from MM-DD-YYYY format to DD-MM-YYYY \n",
    "format. (example \"11-06-2020\" should become \"06-11-2020\"). Optionally*, do the same thing using the `datetime` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c1e48",
   "metadata": {},
   "source": [
    "In this case, in the first function we save the three values that have the - between them in a capturing group, the first has to have two digits, the second too and the third has four digits. After that we just re-write them putting the day before the month.  (\\2-\\1-\\3) <br>\n",
    "The second functions is the same but with the datetime package, we just take the value first and then return it reorganized such thate we put the d (day first, month second, and year third). <br> \n",
    "Source: https://www.progress.com/es/blogs/formato-de-fecha-en-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022e4406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.360858Z",
     "start_time": "2024-10-11T07:03:31.356546Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def switch_md(txt):\n",
    "    aux = re.sub(r'(\\d{2})-(\\d{2})-(\\d{4})', r'\\2-\\1-\\3', txt)\n",
    "    return aux\n",
    "    \n",
    "def switch_md_datetime(txt):\n",
    "    txt = datetime.strptime(txt, '%m-%d-%Y')\n",
    "    return txt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "467bb6f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.399470Z",
     "start_time": "2024-10-11T07:03:31.386752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06-11-2020\n",
      "06-11-2020\n"
     ]
    }
   ],
   "source": [
    "print(switch_md(\"11-06-2020\"))\n",
    "print(switch_md_datetime(\"11-06-2020\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69126e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31-12-2024\n",
      "31-12-2024\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(switch_md(\"12-31-2024\"))\n",
    "print(switch_md_datetime(\"12-31-2024\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b7a58",
   "metadata": {},
   "source": [
    "# Exercice 2 - Analyze XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d48223",
   "metadata": {},
   "source": [
    "- Write a Python code that retrieves the content of the page at:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58360f91",
   "metadata": {},
   "source": [
    "In this case we just use the requests library to get the information from the website and then print it\n",
    "\n",
    "Source: https://j2logo.com/python/python-requests-peticiones-http/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e6710a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.410291Z",
     "start_time": "2024-10-11T07:03:31.400475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<CATALOG>\n",
      "  <CD>\n",
      "    <TITLE>Empire Burlesque</TITLE>\n",
      "    <ARTIST>Bob Dylan</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Columbia</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1985</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Hide your heart</TITLE>\n",
      "    <ARTIST>Bonnie Tyler</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>CBS Records</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1988</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Greatest Hits</TITLE>\n",
      "    <ARTIST>Dolly Parton</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>RCA</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1982</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Still got the blues</TITLE>\n",
      "    <ARTIST>Gary Moore</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Virgin records</COMPANY>\n",
      "    <PRICE>10.20</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Eros</TITLE>\n",
      "    <ARTIST>Eros Ramazzotti</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>BMG</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1997</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>One night only</TITLE>\n",
      "    <ARTIST>Bee Gees</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1998</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Sylvias Mother</TITLE>\n",
      "    <ARTIST>Dr.Hook</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>CBS</COMPANY>\n",
      "    <PRICE>8.10</PRICE>\n",
      "    <YEAR>1973</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Maggie May</TITLE>\n",
      "    <ARTIST>Rod Stewart</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Pickwick</COMPANY>\n",
      "    <PRICE>8.50</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Romanza</TITLE>\n",
      "    <ARTIST>Andrea Bocelli</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>10.80</PRICE>\n",
      "    <YEAR>1996</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>When a man loves a woman</TITLE>\n",
      "    <ARTIST>Percy Sledge</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Atlantic</COMPANY>\n",
      "    <PRICE>8.70</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Black angel</TITLE>\n",
      "    <ARTIST>Savage Rose</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Mega</COMPANY>\n",
      "    <PRICE>10.90</PRICE>\n",
      "    <YEAR>1995</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>1999 Grammy Nominees</TITLE>\n",
      "    <ARTIST>Many</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Grammy</COMPANY>\n",
      "    <PRICE>10.20</PRICE>\n",
      "    <YEAR>1999</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>For the good times</TITLE>\n",
      "    <ARTIST>Kenny Rogers</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Mucik Master</COMPANY>\n",
      "    <PRICE>8.70</PRICE>\n",
      "    <YEAR>1995</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Big Willie style</TITLE>\n",
      "    <ARTIST>Will Smith</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Columbia</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1997</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Tupelo Honey</TITLE>\n",
      "    <ARTIST>Van Morrison</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Polydor</COMPANY>\n",
      "    <PRICE>8.20</PRICE>\n",
      "    <YEAR>1971</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Soulsville</TITLE>\n",
      "    <ARTIST>Jorn Hoel</ARTIST>\n",
      "    <COUNTRY>Norway</COUNTRY>\n",
      "    <COMPANY>WEA</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1996</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>The very best of</TITLE>\n",
      "    <ARTIST>Cat Stevens</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Island</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1990</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Stop</TITLE>\n",
      "    <ARTIST>Sam Brown</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>A and M</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1988</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Bridge of Spies</TITLE>\n",
      "    <ARTIST>T'Pau</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Siren</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Private Dancer</TITLE>\n",
      "    <ARTIST>Tina Turner</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>Capitol</COMPANY>\n",
      "    <PRICE>8.90</PRICE>\n",
      "    <YEAR>1983</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Midt om natten</TITLE>\n",
      "    <ARTIST>Kim Larsen</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Medley</COMPANY>\n",
      "    <PRICE>7.80</PRICE>\n",
      "    <YEAR>1983</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Pavarotti Gala Concert</TITLE>\n",
      "    <ARTIST>Luciano Pavarotti</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>DECCA</COMPANY>\n",
      "    <PRICE>9.90</PRICE>\n",
      "    <YEAR>1991</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>The dock of the bay</TITLE>\n",
      "    <ARTIST>Otis Redding</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>Stax Records</COMPANY>\n",
      "    <PRICE>7.90</PRICE>\n",
      "    <YEAR>1968</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Picture book</TITLE>\n",
      "    <ARTIST>Simply Red</ARTIST>\n",
      "    <COUNTRY>EU</COUNTRY>\n",
      "    <COMPANY>Elektra</COMPANY>\n",
      "    <PRICE>7.20</PRICE>\n",
      "    <YEAR>1985</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Red</TITLE>\n",
      "    <ARTIST>The Communards</ARTIST>\n",
      "    <COUNTRY>UK</COUNTRY>\n",
      "    <COMPANY>London</COMPANY>\n",
      "    <PRICE>7.80</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "  <CD>\n",
      "    <TITLE>Unchain my heart</TITLE>\n",
      "    <ARTIST>Joe Cocker</ARTIST>\n",
      "    <COUNTRY>USA</COUNTRY>\n",
      "    <COMPANY>EMI</COMPANY>\n",
      "    <PRICE>8.20</PRICE>\n",
      "    <YEAR>1987</YEAR>\n",
      "  </CD>\n",
      "</CATALOG>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "page = requests.get('https://www.w3schools.com/xml/cd_catalog.xml')\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28433bc",
   "metadata": {},
   "source": [
    "- Look at the text content and load as xml."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66191d1b",
   "metadata": {},
   "source": [
    "In this section we also get the information from the website but instead of printing it right ahead we give some format to it.\n",
    "\n",
    "First, we ensure that the information was succesfully saved, then according to (1) we convert the information to an element tree, and we create a list with a tuple with our element tree and its level.\n",
    "\n",
    "Then, while there are still elements on our tree we take the last element of our list, and with the first label we print the name of the label of our actual element  and text (if it is not none we apply the strip method to remove blank spaces, in the other case we print None). On the last part we add the childs of the actual element \n",
    "\n",
    "Source: https://docs.python.org/es/3/library/xml.etree.elementtree.html (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce02d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATALOG: \n",
      "CD: \n",
      "TITLE: Empire Burlesque\n",
      "ARTIST: Bob Dylan\n",
      "COUNTRY: USA\n",
      "COMPANY: Columbia\n",
      "PRICE: 10.90\n",
      "YEAR: 1985\n",
      "CD: \n",
      "TITLE: Hide your heart\n",
      "ARTIST: Bonnie Tyler\n",
      "COUNTRY: UK\n",
      "COMPANY: CBS Records\n",
      "PRICE: 9.90\n",
      "YEAR: 1988\n",
      "CD: \n",
      "TITLE: Greatest Hits\n",
      "ARTIST: Dolly Parton\n",
      "COUNTRY: USA\n",
      "COMPANY: RCA\n",
      "PRICE: 9.90\n",
      "YEAR: 1982\n",
      "CD: \n",
      "TITLE: Still got the blues\n",
      "ARTIST: Gary Moore\n",
      "COUNTRY: UK\n",
      "COMPANY: Virgin records\n",
      "PRICE: 10.20\n",
      "YEAR: 1990\n",
      "CD: \n",
      "TITLE: Eros\n",
      "ARTIST: Eros Ramazzotti\n",
      "COUNTRY: EU\n",
      "COMPANY: BMG\n",
      "PRICE: 9.90\n",
      "YEAR: 1997\n",
      "CD: \n",
      "TITLE: One night only\n",
      "ARTIST: Bee Gees\n",
      "COUNTRY: UK\n",
      "COMPANY: Polydor\n",
      "PRICE: 10.90\n",
      "YEAR: 1998\n",
      "CD: \n",
      "TITLE: Sylvias Mother\n",
      "ARTIST: Dr.Hook\n",
      "COUNTRY: UK\n",
      "COMPANY: CBS\n",
      "PRICE: 8.10\n",
      "YEAR: 1973\n",
      "CD: \n",
      "TITLE: Maggie May\n",
      "ARTIST: Rod Stewart\n",
      "COUNTRY: UK\n",
      "COMPANY: Pickwick\n",
      "PRICE: 8.50\n",
      "YEAR: 1990\n",
      "CD: \n",
      "TITLE: Romanza\n",
      "ARTIST: Andrea Bocelli\n",
      "COUNTRY: EU\n",
      "COMPANY: Polydor\n",
      "PRICE: 10.80\n",
      "YEAR: 1996\n",
      "CD: \n",
      "TITLE: When a man loves a woman\n",
      "ARTIST: Percy Sledge\n",
      "COUNTRY: USA\n",
      "COMPANY: Atlantic\n",
      "PRICE: 8.70\n",
      "YEAR: 1987\n",
      "CD: \n",
      "TITLE: Black angel\n",
      "ARTIST: Savage Rose\n",
      "COUNTRY: EU\n",
      "COMPANY: Mega\n",
      "PRICE: 10.90\n",
      "YEAR: 1995\n",
      "CD: \n",
      "TITLE: 1999 Grammy Nominees\n",
      "ARTIST: Many\n",
      "COUNTRY: USA\n",
      "COMPANY: Grammy\n",
      "PRICE: 10.20\n",
      "YEAR: 1999\n",
      "CD: \n",
      "TITLE: For the good times\n",
      "ARTIST: Kenny Rogers\n",
      "COUNTRY: UK\n",
      "COMPANY: Mucik Master\n",
      "PRICE: 8.70\n",
      "YEAR: 1995\n",
      "CD: \n",
      "TITLE: Big Willie style\n",
      "ARTIST: Will Smith\n",
      "COUNTRY: USA\n",
      "COMPANY: Columbia\n",
      "PRICE: 9.90\n",
      "YEAR: 1997\n",
      "CD: \n",
      "TITLE: Tupelo Honey\n",
      "ARTIST: Van Morrison\n",
      "COUNTRY: UK\n",
      "COMPANY: Polydor\n",
      "PRICE: 8.20\n",
      "YEAR: 1971\n",
      "CD: \n",
      "TITLE: Soulsville\n",
      "ARTIST: Jorn Hoel\n",
      "COUNTRY: Norway\n",
      "COMPANY: WEA\n",
      "PRICE: 7.90\n",
      "YEAR: 1996\n",
      "CD: \n",
      "TITLE: The very best of\n",
      "ARTIST: Cat Stevens\n",
      "COUNTRY: UK\n",
      "COMPANY: Island\n",
      "PRICE: 8.90\n",
      "YEAR: 1990\n",
      "CD: \n",
      "TITLE: Stop\n",
      "ARTIST: Sam Brown\n",
      "COUNTRY: UK\n",
      "COMPANY: A and M\n",
      "PRICE: 8.90\n",
      "YEAR: 1988\n",
      "CD: \n",
      "TITLE: Bridge of Spies\n",
      "ARTIST: T'Pau\n",
      "COUNTRY: UK\n",
      "COMPANY: Siren\n",
      "PRICE: 7.90\n",
      "YEAR: 1987\n",
      "CD: \n",
      "TITLE: Private Dancer\n",
      "ARTIST: Tina Turner\n",
      "COUNTRY: UK\n",
      "COMPANY: Capitol\n",
      "PRICE: 8.90\n",
      "YEAR: 1983\n",
      "CD: \n",
      "TITLE: Midt om natten\n",
      "ARTIST: Kim Larsen\n",
      "COUNTRY: EU\n",
      "COMPANY: Medley\n",
      "PRICE: 7.80\n",
      "YEAR: 1983\n",
      "CD: \n",
      "TITLE: Pavarotti Gala Concert\n",
      "ARTIST: Luciano Pavarotti\n",
      "COUNTRY: UK\n",
      "COMPANY: DECCA\n",
      "PRICE: 9.90\n",
      "YEAR: 1991\n",
      "CD: \n",
      "TITLE: The dock of the bay\n",
      "ARTIST: Otis Redding\n",
      "COUNTRY: USA\n",
      "COMPANY: Stax Records\n",
      "PRICE: 7.90\n",
      "YEAR: 1968\n",
      "CD: \n",
      "TITLE: Picture book\n",
      "ARTIST: Simply Red\n",
      "COUNTRY: EU\n",
      "COMPANY: Elektra\n",
      "PRICE: 7.20\n",
      "YEAR: 1985\n",
      "CD: \n",
      "TITLE: Red\n",
      "ARTIST: The Communards\n",
      "COUNTRY: UK\n",
      "COMPANY: London\n",
      "PRICE: 7.80\n",
      "YEAR: 1987\n",
      "CD: \n",
      "TITLE: Unchain my heart\n",
      "ARTIST: Joe Cocker\n",
      "COUNTRY: USA\n",
      "COMPANY: EMI\n",
      "PRICE: 8.20\n",
      "YEAR: 1987\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "page = requests.get('https://www.w3schools.com/xml/cd_catalog.xml')\n",
    "\n",
    "if page.status_code == 200:\n",
    "    root = ET.fromstring(page.content)\n",
    "    stack = [(root, 0)] \n",
    "    while stack:\n",
    "        elem, level = stack.pop()\n",
    "        print(f\"{elem.tag}: {elem.text.strip() if elem.text else 'None'}\")\n",
    "        stack.extend((child, level + 1) for child in reversed(list(elem)))\n",
    "    \n",
    "else:\n",
    "    print(f\"Error: {page.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbddf61",
   "metadata": {},
   "source": [
    "- Write a `display_cd` function that displays (i.e. `print`), for a CD: title, artist, country, company, year.\n",
    "- Display all CDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de7cd6",
   "metadata": {},
   "source": [
    "In this part, we first search for all the CD elements inside root (in this case the findall function will give us the full list). after this, we get the information from each label and save it in a variable. After that, we return the string with all the information, and then print the result. We used both the following websites (1) and (2) to do so.\n",
    "\n",
    "Sources: <br> https://www.w3schools.com/python/ref_string_find.asp (1) <br> https://www.codecademy.com/resources/docs/python/regex/findall (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b026e3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.730469Z",
     "start_time": "2024-10-11T07:03:31.712761Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empire Burlesque, Bob Dylan, USA, Columbia, 1985\n",
      "Hide your heart, Bonnie Tyler, UK, CBS Records, 1988\n",
      "Greatest Hits, Dolly Parton, USA, RCA, 1982\n",
      "Still got the blues, Gary Moore, UK, Virgin records, 1990\n",
      "Eros, Eros Ramazzotti, EU, BMG, 1997\n",
      "One night only, Bee Gees, UK, Polydor, 1998\n",
      "Sylvias Mother, Dr.Hook, UK, CBS, 1973\n",
      "Maggie May, Rod Stewart, UK, Pickwick, 1990\n",
      "Romanza, Andrea Bocelli, EU, Polydor, 1996\n",
      "When a man loves a woman, Percy Sledge, USA, Atlantic, 1987\n",
      "Black angel, Savage Rose, EU, Mega, 1995\n",
      "1999 Grammy Nominees, Many, USA, Grammy, 1999\n",
      "For the good times, Kenny Rogers, UK, Mucik Master, 1995\n",
      "Big Willie style, Will Smith, USA, Columbia, 1997\n",
      "Tupelo Honey, Van Morrison, UK, Polydor, 1971\n",
      "Soulsville, Jorn Hoel, Norway, WEA, 1996\n",
      "The very best of, Cat Stevens, UK, Island, 1990\n",
      "Stop, Sam Brown, UK, A and M, 1988\n",
      "Bridge of Spies, T'Pau, UK, Siren, 1987\n",
      "Private Dancer, Tina Turner, UK, Capitol, 1983\n",
      "Midt om natten, Kim Larsen, EU, Medley, 1983\n",
      "Pavarotti Gala Concert, Luciano Pavarotti, UK, DECCA, 1991\n",
      "The dock of the bay, Otis Redding, USA, Stax Records, 1968\n",
      "Picture book, Simply Red, EU, Elektra, 1985\n",
      "Red, The Communards, UK, London, 1987\n",
      "Unchain my heart, Joe Cocker, USA, EMI, 1987\n"
     ]
    }
   ],
   "source": [
    "all_cds = root.findall('CD')\n",
    "\n",
    "def display_cd(cd_element):\n",
    "    title = cd_element.find('TITLE').text\n",
    "    artist = cd_element.find('ARTIST').text\n",
    "    country = cd_element.find('COUNTRY').text \n",
    "    company = cd_element.find('COMPANY').text \n",
    "    year = cd_element.find('YEAR').text\n",
    "    \n",
    "    return title + \", \" + artist + \", \" + country + \", \" +  company + \", \" +  year\n",
    "\n",
    "print('\\n'.join([display_cd(cd) for cd in all_cds]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab61ae",
   "metadata": {},
   "source": [
    "- Display all 1980s CDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718d9a5",
   "metadata": {},
   "source": [
    "For this question, we extract the year of each cd, and we just print the entry cd only if the year is between 1980 and 1989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d5ad903",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empire Burlesque, Bob Dylan, USA, Columbia, 1985\n",
      "Hide your heart, Bonnie Tyler, UK, CBS Records, 1988\n",
      "Greatest Hits, Dolly Parton, USA, RCA, 1982\n",
      "When a man loves a woman, Percy Sledge, USA, Atlantic, 1987\n",
      "Stop, Sam Brown, UK, A and M, 1988\n",
      "Bridge of Spies, T'Pau, UK, Siren, 1987\n",
      "Private Dancer, Tina Turner, UK, Capitol, 1983\n",
      "Midt om natten, Kim Larsen, EU, Medley, 1983\n",
      "Picture book, Simply Red, EU, Elektra, 1985\n",
      "Red, The Communards, UK, London, 1987\n",
      "Unchain my heart, Joe Cocker, USA, EMI, 1987\n"
     ]
    }
   ],
   "source": [
    "for cd in all_cds:\n",
    "    year = cd.find('YEAR').text\n",
    "    if year <= '1989' and year >= '1980':\n",
    "        print('\\n'.join([display_cd(cd)])) \n",
    "# take all the elements with the value year between 1980 and 1989"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52829c4",
   "metadata": {},
   "source": [
    "- Display all British CDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9445ac45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hide your heart, Bonnie Tyler, UK, CBS Records, 1988\n",
      "Still got the blues, Gary Moore, UK, Virgin records, 1990\n",
      "One night only, Bee Gees, UK, Polydor, 1998\n",
      "Sylvias Mother, Dr.Hook, UK, CBS, 1973\n",
      "Maggie May, Rod Stewart, UK, Pickwick, 1990\n",
      "For the good times, Kenny Rogers, UK, Mucik Master, 1995\n",
      "Tupelo Honey, Van Morrison, UK, Polydor, 1971\n",
      "The very best of, Cat Stevens, UK, Island, 1990\n",
      "Stop, Sam Brown, UK, A and M, 1988\n",
      "Bridge of Spies, T'Pau, UK, Siren, 1987\n",
      "Private Dancer, Tina Turner, UK, Capitol, 1983\n",
      "Pavarotti Gala Concert, Luciano Pavarotti, UK, DECCA, 1991\n",
      "Red, The Communards, UK, London, 1987\n"
     ]
    }
   ],
   "source": [
    "for cd in all_cds:\n",
    "    Country = cd.find('COUNTRY').text\n",
    "    if Country == 'UK':\n",
    "        print('\\n'.join([display_cd(cd)])) \n",
    "# Similarly to the question above but with filtering only the cds with the value uk in their country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8498b203",
   "metadata": {},
   "source": [
    "# Exercice 3 - Analyze JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099159f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Write a Python program that gets the file of filming locations in Paris at:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0472b7",
   "metadata": {},
   "source": [
    "First, we create a session and use a get request on the url to get the filming locations. The response r has the content of the url requested.\n",
    "The response should be parsed into json with r.json() because the GET request returned JSON data, so .json() function allows us to convert that JSON data into a Python object <br>\n",
    "Source: https://www.geeksforgeeks.org/response-json-python-requests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a0ef9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:31.786738Z",
     "start_time": "2024-10-11T07:03:31.778307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12265\n",
      "{'datasetid': 'lieux-de-tournage-a-paris', 'recordid': '47c4013d8ee78ed66c9d2a7bda15e37387e3088b', 'fields': {'coord_x': 2.39934074, 'id_lieu': '2019-239', 'adresse_lieu': '206 avenue daumesnil, 75012 paris', 'geo_shape': {'coordinates': [2.3993407414840595, 48.83798025216695], 'type': 'Point'}, 'coord_y': 48.83798025, 'ardt_lieu': '75012', 'nom_tournage': 'POLICE', 'nom_realisateur': 'ANNE FONTAINE', 'date_debut': '2019-03-08', 'type_tournage': 'Long métrage', 'annee_tournage': '2019', 'nom_producteur': 'F COMME FILM', 'date_fin': '2019-03-09', 'geo_point_2d': [48.83798025216695, 2.3993407414840595]}, 'geometry': {'type': 'Point', 'coordinates': [2.3993407414840595, 48.83798025216695]}, 'record_timestamp': '2024-01-31T13:40:46.402+01:00'}\n"
     ]
    }
   ],
   "source": [
    "from json import load, dump\n",
    "from pathlib import Path\n",
    "from requests import Session\n",
    "\n",
    "url = \"https://opendata.paris.fr/explore/dataset/lieux-de-tournage-a-paris/download/?format=json&timezone=Europe/Berlin&lang=fr\"\n",
    "\n",
    "s = Session()\n",
    "r = s.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    # Storing the JSON data in a variable\n",
    "    locs = r.json()  # Parsing the JSON data\n",
    "else:\n",
    "    locs = None\n",
    "\n",
    "print(len(locs))\n",
    "\n",
    "print(locs[0])\n",
    "# print(locs[0].get(\"fields\", \"[Unknown]\").get(\"nom_tournage\", \"[Unknown]\"))\n",
    "# print(type(locs[0]))\n",
    "# print(locs[0].keys())\n",
    "# print(locs[0]['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec0630",
   "metadata": {},
   "source": [
    "- How many entries have you got?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ce8cc",
   "metadata": {},
   "source": [
    "We obtained 12265 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ab4ed",
   "metadata": {},
   "source": [
    "- Analyze the JSON file: what is its structure?\n",
    "- Write a function that converts an entry in a string that shows director, title, district, start date, end date, and geographic coordinates.\n",
    "- Convert all entries in strings (warning: some entries may have issues).\n",
    "- Display the first 20 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57634b19",
   "metadata": {},
   "source": [
    "The json file is made of different dictionaries. Each dictionary belongs to one filming location of a movie in Paris. <br>Each dictionary has different key-value pairs that give us details about each filming location. Examples of key-value pairs include: datasetid, recordid, fields, geometry, and record_timestamp which are the keys found at the top level of the json file.\n",
    "\n",
    "The function wants us to print each entry as a string that shows the values of the following keys:\n",
    "* nom_tournage : for the title of the film\n",
    "* nom_realisateur : for the name of the director\n",
    "* date_debut : for the start date\n",
    "* date_fin : for the end date\n",
    "* ardt_lieu : for the post code\n",
    "* coordinates (in 'geometry' key) : for the geographic coordinates\n",
    "\n",
    "We figured that the function \"get\" in dictionaries help a lot for extracting specific values of keys, especially in cases where some entries are missing some values. This could be handled by using the get function (1) and supplying it with a second parameter which is the value we want to assign to a missing value of an entry. we chose the second parameter to be [Unknown]. This way, if for example a certain entry is missing the name of the director, it would say [unknown] for the director name.\n",
    "\n",
    "Sources we used to manipulate dictionaries more comfortably: <br> https://www.geeksforgeeks.org/handling-missing-keys-python-dictionaries/ (1)<br> https://www.geeksforgeeks.org/iterate-over-a-dictionary-in-python/ (2) <br>\n",
    "https://note.nkmk.me/en/python-dict-keys-values-items/ (3)<br>\n",
    "https://www.w3schools.com/python/python_ref_dictionary.asp (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb3e7f",
   "metadata": {},
   "source": [
    "We also used the title() function for the movie title and director to make them all start with a capital letter.\n",
    "\n",
    "Source: https://www.toppr.com/guides/python-guide/references/methods-and-functions/methods/string/title/python-string-title/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299da1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.576923Z",
     "start_time": "2024-10-11T07:03:35.572252Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_loc(entry):\n",
    "    # the entry is a dictionary.\n",
    "    # we extract the key values that we are interested in printing, namely: title, director, start and end dates, post code, and coordinates.\n",
    "    title=entry.get(\"fields\", \"[Unknown]\").get(\"nom_tournage\", \"[Unknown]\").title()\n",
    "    director=entry.get(\"fields\", \"[Unknown]\").get(\"nom_realisateur\", \"[Unknown]\").title()\n",
    "    start_date=entry.get(\"fields\", \"[Unknown]\").get(\"date_debut\", \"[Unknown]\")\n",
    "    end_date=entry.get(\"fields\", \"[Unknown]\").get(\"date_fin\", \"[Unknown]\")\n",
    "    code=entry.get(\"fields\", \"[Unknown]\").get(\"ardt_lieu\", \"[Unknown]\")\n",
    "    coordinates=entry.get(\"geometry\", \"[Unknown]\").get(\"coordinates\", \"[Unknown]\")\n",
    "    # separating the coordinates array that we get into two elements and handling the case where the value of the key \"coordinates\" is unknown or missing from our dictionary\n",
    "    if(coordinates!=\"[Unknown]\"):\n",
    "        coordinate1=str(coordinates[0])\n",
    "        coordinate2=str(coordinates[1])\n",
    "    else: \n",
    "        coordinate1=\"Unknown\"\n",
    "        coordinate2=\"Unknown\"\n",
    "    # saving the details into a string that we want to display later according to the format asked.\n",
    "    s=\"\\\"\"\n",
    "    s+=title\n",
    "    s+=\"\\\"\"\n",
    "    s+=\", by \"\n",
    "    s+=director\n",
    "    s+=\", from \"\n",
    "    s+=start_date\n",
    "    s+=\" to \"\n",
    "    s+=end_date\n",
    "    s+=\", in \"\n",
    "    s+=code\n",
    "    s+=\" ([\"\n",
    "    s+=coordinate1\n",
    "    s+=\", \"\n",
    "    s+=coordinate2\n",
    "    s+=\"])\"\n",
    "\n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bfa5a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.670903Z",
     "start_time": "2024-10-11T07:03:35.614408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Police\", by Anne Fontaine, from 2019-03-08 to 2019-03-09, in 75012 ([2.3993407414840595, 48.83798025216695])\n",
      "\"L'Attaché\", by Eli Ben-David, from 2019-03-14 to 2019-03-14, in 75018 ([2.3444346117556014, 48.88730126357826])\n",
      "\"En Attendant Qui ? Mai\", by Marc Recuenco, from 2019-06-11 to 2019-06-11, in 75017 ([2.3059527844268386, 48.883564598239715])\n",
      "\"Tout Simplement Noir\", by Jean Pascal Zadi Et John Waxxx, from 2019-05-23 to 2019-05-23, in 75005 ([2.350245473422228, 48.84859141991906])\n",
      "\"Une Famille Formidable\", by Nicolas Herdt, from 2018-08-06 to 2018-08-06, in 75003 ([2.363650294215209, 48.860250402104214])\n",
      "\"Une Famille Formidable\", by Nicolas Herdt, from 2018-08-06 to 2018-08-06, in 75003 ([2.362155495714307, 48.86295435066187])\n",
      "\"Les Mignonnes\", by Maïmouna Doucouré, from 2018-08-07 to 2018-08-07, in 75019 ([2.3820880704061613, 48.88213499024688])\n",
      "\"Lebowitz Contre Lebowitz/9 A 12\", by Christophe Barraud, from 2016-11-09 to 2016-11-09, in 75013 ([2.3593549995397, 48.8387789995482])\n",
      "\"Leo Mattei/14 Et 15\", by Nicolas Herdt, from 2016-10-06 to 2016-10-06, in 75004 ([2.36566899993831, 48.84726000024456])\n",
      "\"10%/Saison 2\", by Jeanne Herry, from 2016-09-14 to 2016-09-14, in 75019 ([2.3846879994691474, 48.87377700000735])\n",
      "\"War Machine\", by David Michod, from 2016-01-30 to 2016-01-30, in 75001 ([2.338080999447199, 48.86186300028194])\n",
      "\"Stella Est Amoureuse\", by Sylvie Verheyde, from 2020-12-22 to 2020-12-22, in 75013 ([2.3507621803435717, 48.828828099592])\n",
      "\"Stella Est Amoureuse\", by Sylvie Verheyde, from 2020-12-22 to 2020-12-22, in 75013 ([2.3513781872736765, 48.82793217601982])\n",
      "\"Stella Est Amoureuse\", by Sylvie Verheyde, from 2020-12-22 to 2020-12-22, in 75013 ([2.3513781872736765, 48.82793217601982])\n",
      "\"Stella Est Amoureuse\", by Sylvie Verheyde, from 2020-12-22 to 2020-12-22, in 75001 ([2.3474874555972582, 48.86046194822874])\n",
      "\"Stella Est Amoureuse\", by Sylvie Verheyde, from 2020-12-22 to 2020-12-22, in 75001 ([2.3477222716177035, 48.86094046750017])\n",
      "\"J'Accuse\", by Roman Polanski, from 2019-02-25 to 2019-02-25, in 75015 ([2.3099660729815525, 48.84698000789973])\n",
      "\"Le Bazar De La Charité\", by Alexandre Laurent, from 2019-03-14 to 2019-03-15, in 75008 ([2.3124555080176545, 48.87963776953965])\n",
      "\"Le Bazar De La Charité\", by Alexandre Laurent, from 2019-03-18 to 2019-03-18, in 75008 ([2.311239089679555, 48.88051651542833])\n",
      "\"Les Heroiques\", by Maxime Roy, from 2019-03-11 to 2019-03-12, in 75010 ([2.361435072157541, 48.88235203765779])\n"
     ]
    }
   ],
   "source": [
    "# converting all entries into strings using the function display_loc that we created:\n",
    "all_entries = [display_loc(e) for e in locs]\n",
    "# Printing only the first 20 entries in the format asked:\n",
    "print('\\n'.join(all_entries[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fe95c",
   "metadata": {},
   "source": [
    "- A same movie can have multiple shooting locations. Make a list of movies, where each entry contains the movie title, its director, and shootings locations (district, start date, end date).\n",
    "- How many movies do you have?\n",
    "- Write a function that converts a movie into a string that shows director, title, and shootings.\n",
    "- Convert all movies in strings.\n",
    "- Display the first 20 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041bc71b",
   "metadata": {},
   "source": [
    "For this question, the format of the dictionary we decided on is the following:\n",
    "\n",
    "movies = {'movie_title':{ <br>\n",
    "           'director': director_name, <br>\n",
    "           'shooting_locations': [{ 'district': district_nbr, 'start_date': start_date, 'end_date': end_date }, <br>\n",
    "           {'district': district_nbr, 'start_date': start_date, 'end_date': end_date}, <br>\n",
    "           {'district': district_nbr, 'start_date': start_date, 'end_date': end_date}, ...] <br>\n",
    "}} <br>\n",
    "Each movie title is a key and the value is a dictionary having the name of the director of the movie, and a shooting locations key where the value is an array of dictionaries of each shooting location of that movie. each shooting location has information about the district, start_date, and end_date of that shooting location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50addd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we created a function exists which takes as parameters a dictionary and a key. it returns true if the key exists in the dictionary, false otherwise\n",
    "def exists(dictionary, key):\n",
    "    if len(dictionary.keys())==0:\n",
    "        return False\n",
    "    for k in dictionary.keys():\n",
    "        if k==key:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3d7fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.675898Z",
     "start_time": "2024-10-11T07:03:35.672816Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "movies = dict()\n",
    "\n",
    "# First, we loop \"locs\", the list of dictionaries of each movie\n",
    "# An entry belongs to a movie with information about its title, director, and one shooting location where the movie took place.\n",
    "for entry in locs:\n",
    "    # From the entry get the title of the movie, the director, the district, start date and end date while placing an \"[Unknown]\" value wherever the value of the key we want is not present in the original list\n",
    "    fields=entry.get(\"fields\")\n",
    "    title=fields.get(\"nom_tournage\").title()\n",
    "    director= fields.get(\"nom_realisateur\", \"[Unknown]\").title()\n",
    "    district= fields.get(\"ardt_lieu\",\"[Unknown]\")\n",
    "    start_date= fields.get(\"date_debut\", \"[Unknown]\")\n",
    "    end_date= fields.get(\"date_fin\", \"[Unknown]\")\n",
    "    # we create a new_location dictionary which contains the values for district, start date, and end date.\n",
    "    new_location={\"district\":district, \"start_date\": start_date, \"end_date\": end_date }\n",
    "\n",
    "    # if the movie title of the entry does not exist in our movies dictionary, we want to add it in the format specified above. \n",
    "    # meaning: movie_title as the key, and values are the director, and shooting_locations array. it's an array that contains only one element, meaning one shooting location of the entry (with information about the district start date and end date),\n",
    "    # to which we will add more shooting locations later on ( after entering it in the movies dictionary first). \n",
    "    if exists(movies,title)==False:\n",
    "        # if movie title is not in our list, add it to the list for the first time.\n",
    "        shooting_locations=[]\n",
    "        shooting_locations.append(new_location)\n",
    "        movie={\n",
    "            \"director\": director,\n",
    "            \"shooting_locations\": shooting_locations\n",
    "        }          \n",
    "        movies.update({title: movie}) # this adds the key-value pair we created above to our dictionary movies.\n",
    "    else:\n",
    "        # that means the movie is already in our list and we encountered a new shooting location for that movie\n",
    "        # we need to add a new shooting location new_location to our already existing movie entry: we apend it in the movies dictionary to the shooting_locations array of THAT movie title exclusively\n",
    "        movies[title][\"shooting_locations\"].append(new_location)\n",
    "\n",
    "\n",
    "# print(movies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30c236",
   "metadata": {},
   "source": [
    "From printing the length of the movies dictionary we created, we can see that we have 1476 different movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1e6d3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.728960Z",
     "start_time": "2024-10-11T07:03:35.721811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1389"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8fadc13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.740289Z",
     "start_time": "2024-10-11T07:03:35.730968Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def display_movie(movie):\n",
    "    \n",
    "    # each movie is an element in the movies list that has the movie title as the first element, and a dictionary of the director name and shooting locations array as the second element.\n",
    "\n",
    "    # we get the values of the title, director and shooting_locations array and we create the string having the format that is asked.\n",
    "    title=movie[0]\n",
    "    director=movie[1]['director']\n",
    "    shootings= movie[1]['shooting_locations'] #array that we have to loop\n",
    "    s=\"\\\"\"+title+\"\\\"\"\n",
    "    s+=\", by \"\n",
    "    s+=director\n",
    "    s+=\". Shootings: \"\n",
    "    for loc in shootings:\n",
    "        district=loc[\"district\"]\n",
    "        start_date=loc[\"start_date\"]\n",
    "        end_date=loc[\"end_date\"]\n",
    "        s+=district+\" from \"\n",
    "        s+=start_date + \" to \"\n",
    "        s+=end_date+\"; \"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d852fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.773580Z",
     "start_time": "2024-10-11T07:03:35.753086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Police\", by Anne Fontaine. Shootings: 75012 from 2019-03-08 to 2019-03-09; 75012 from 2019-03-08 to 2019-03-09; 75012 from 2019-04-10 to 2019-04-11; 75012 from 2019-03-11 to 2019-03-12; 75020 from 2019-03-27 to 2019-03-27; 75012 from 2019-03-07 to 2019-03-08; 75011 from 2019-03-27 to 2019-03-27; 75019 from 2019-03-28 to 2019-03-28; 75012 from 2019-03-25 to 2019-03-25; 75012 from 2019-03-28 to 2019-03-28; 75019 from 2019-04-08 to 2019-04-09; \n",
      "\"L'Attaché\", by Eli Ben-David. Shootings: 75018 from 2019-03-14 to 2019-03-14; 75018 from 2019-03-14 to 2019-03-14; 75005 from 2019-03-15 to 2019-03-15; 75012 from 2019-03-12 to 2019-03-12; 75009 from 2019-03-12 to 2019-03-12; 75001 from 2019-03-12 to 2019-03-12; 75004 from 2019-03-20 to 2019-03-20; 75001 from 2019-03-15 to 2019-03-16; 75004 from 2019-03-16 to 2019-03-16; 75005 from 2019-03-12 to 2019-03-12; 75010 from 2019-03-12 to 2019-03-12; 75007 from 2019-03-13 to 2019-03-13; 75010 from 2019-03-12 to 2019-03-12; 75004 from 2019-03-16 to 2019-03-16; 75018 from 2019-03-14 to 2019-03-15; 75003 from 2019-03-16 to 2019-03-16; 75018 from 2019-03-14 to 2019-03-14; 75001 from 2019-03-21 to 2019-03-22; 75004 from 2019-03-22 to 2019-03-23; \n",
      "\"En Attendant Qui ? Mai\", by Marc Recuenco. Shootings: 75017 from 2019-06-11 to 2019-06-11; 75017 from 2019-06-10 to 2019-06-10; 75017 from 2019-06-15 to 2019-06-15; 75017 from 2019-06-22 to 2019-06-23; 75005 from 2019-06-17 to 2019-06-18; 75005 from 2019-06-23 to 2019-06-23; 75017 from 2019-06-13 to 2019-06-13; 75017 from 2019-06-06 to 2019-06-07; 75013 from 2019-06-22 to 2019-06-22; 75017 from 2019-06-15 to 2019-06-15; 75017 from 2019-06-22 to 2019-06-23; 75017 from 2019-06-11 to 2019-06-11; 75017 from 2019-06-07 to 2019-06-07; 75017 from 2019-06-11 to 2019-06-11; 75017 from 2019-06-22 to 2019-06-23; 75005 from 2019-06-17 to 2019-06-17; \n",
      "\"Tout Simplement Noir\", by Jean Pascal Zadi Et John Waxxx. Shootings: 75005 from 2019-05-23 to 2019-05-23; 75008 from 2019-06-12 to 2019-06-12; 75010 from 2019-05-30 to 2019-05-30; 75019 from 2019-06-26 to 2019-06-26; 75008 from 2019-06-13 to 2019-06-13; 75020 from 2019-06-04 to 2019-06-04; 75001 from 2019-06-28 to 2019-06-28; 75015 from 2019-07-05 to 2019-07-05; 75007 from 2019-07-05 to 2019-07-05; 75002 from 2019-06-16 to 2019-06-17; 75020 from 2019-06-04 to 2019-06-04; 75007 from 2019-05-30 to 2019-05-31; 75002 from 2019-06-26 to 2019-06-26; 75009 from 2019-06-24 to 2019-06-24; 75011 from 2019-07-10 to 2019-07-10; 75007 from 2019-07-05 to 2019-07-05; 75007 from 2019-05-31 to 2019-05-31; 75001 from 2019-06-26 to 2019-06-26; 75015 from 2019-06-03 to 2019-06-03; 75015 from 2019-07-05 to 2019-07-05; 75002 from 2019-06-12 to 2019-06-12; 75017 from 2019-06-25 to 2019-06-25; 75006 from 2019-07-10 to 2019-07-11; 75007 from 2019-07-05 to 2019-07-05; 75005 from 2019-05-23 to 2019-05-23; 75009 from 2019-05-22 to 2019-05-23; 75008 from 2019-06-28 to 2019-06-28; 75004 from 2019-05-23 to 2019-05-23; 75002 from 2019-06-24 to 2019-06-24; 75116 from 2019-06-28 to 2019-06-28; 75010 from 2019-05-29 to 2019-05-30; 75002 from 2019-06-12 to 2019-06-12; 75002 from 2019-06-24 to 2019-06-24; 75001 from 2019-06-24 to 2019-06-24; 75007 from 2019-05-31 to 2019-05-31; 75004 from 2019-05-20 to 2019-05-20; 75009 from 2019-06-12 to 2019-06-12; \n",
      "\"Une Famille Formidable\", by Nicolas Herdt. Shootings: 75003 from 2018-08-06 to 2018-08-06; 75003 from 2018-08-06 to 2018-08-06; 75006 from 2018-08-09 to 2018-08-09; 75007 from 2018-08-13 to 2018-08-13; 75010 from 2018-08-14 to 2018-08-14; 75013 from 2018-08-24 to 2018-08-24; 75006 from 2018-08-09 to 2018-08-09; 75006 from 2018-08-31 to 2018-08-31; 75010 from 2018-08-21 to 2018-08-21; 75010 from 2018-08-20 to 2018-08-20; 75006 from 2018-08-31 to 2018-08-31; 75003 from 2018-08-07 to 2018-08-08; 75007 from 2018-08-13 to 2018-08-13; 75020 from 2018-08-28 to 2018-08-29; 75007 from 2018-07-27 to 2018-07-27; 75015 from 2018-08-09 to 2018-08-09; 75015 from 2018-08-08 to 2018-08-09; 75018 from 2018-08-10 to 2018-08-10; 75010 from 2018-08-17 to 2018-08-17; 75010 from 2018-08-16 to 2018-08-16; \n",
      "\"Les Mignonnes\", by Maïmouna Doucouré. Shootings: 75019 from 2018-08-07 to 2018-08-07; 75019 from 2018-08-22 to 2018-08-27; 75019 from 2018-08-06 to 2018-08-06; 75019 from 2018-08-09 to 2018-08-10; 75019 from 2018-08-06 to 2018-08-06; 75019 from 2018-08-06 to 2018-08-06; 75019 from 2018-08-08 to 2018-08-08; 75019 from 2018-08-08 to 2018-08-08; 75019 from 2018-08-22 to 2018-08-27; \n",
      "\"Lebowitz Contre Lebowitz/9 A 12\", by Christophe Barraud. Shootings: 75013 from 2016-11-09 to 2016-11-09; 75018 from 2016-11-04 to 2016-11-04; 75011 from 2016-10-31 to 2016-10-31; 75002 from 2016-11-01 to 2016-11-01; 75018 from 2016-11-04 to 2016-11-04; 75011 from 2016-11-02 to 2016-11-02; 75013 from 2016-11-09 to 2016-11-09; 75011 from 2016-10-31 to 2016-10-31; 75010 from 2016-10-27 to 2016-10-27; 75010 from 2016-10-27 to 2016-10-27; 75010 from 2016-10-27 to 2016-10-27; 75004 from 2016-10-24 to 2016-10-24; 75016 from 2016-10-25 to 2016-10-26; 75018 from 2016-11-04 to 2016-11-04; 75018 from 2016-11-04 to 2016-11-04; \n",
      "\"Leo Mattei/14 Et 15\", by Nicolas Herdt. Shootings: 75004 from 2016-10-06 to 2016-10-06; 75003 from 2016-10-05 to 2016-10-05; 75004 from 2016-10-06 to 2016-10-06; 75003 from 2016-10-05 to 2016-10-05; 75010 from 2016-10-18 to 2016-10-18; 75010 from 2016-10-18 to 2016-10-18; 75003 from 2016-10-05 to 2016-10-05; 75012 from 2016-10-07 to 2016-10-07; 75009 from 2016-10-01 to 2016-10-01; 75004 from 2016-10-06 to 2016-10-06; \n",
      "\"10%/Saison 2\", by Jeanne Herry. Shootings: 75019 from 2016-09-14 to 2016-09-14; 75001 from 2016-09-05 to 2016-09-05; 75019 from 2016-10-03 to 2016-10-03; 75008 from 2016-09-12 to 2016-09-12; 75019 from 2016-10-03 to 2016-10-03; 75008 from 2016-09-13 to 2016-09-13; 75016 from 2016-10-13 to 2016-10-13; 75007 from 2016-10-10 to 2016-10-10; 75009 from 2016-09-06 to 2016-09-06; 75001 from 2016-09-05 to 2016-09-05; 75009 from 2016-09-29 to 2016-09-29; 75019 from 2016-10-03 to 2016-10-03; 75007 from 2016-10-10 to 2016-10-10; 75001 from 2016-09-05 to 2016-09-05; 75007 from 2016-10-10 to 2016-10-10; 75008 from 2016-10-17 to 2016-10-18; 75001 from 2016-09-05 to 2016-09-05; 75007 from 2016-10-10 to 2016-10-10; 75020 from 2016-09-30 to 2016-09-30; 75009 from 2016-09-29 to 2016-09-29; 75019 from 2016-10-03 to 2016-10-03; 75020 from 2016-09-30 to 2016-09-30; 75008 from 2016-10-14 to 2016-10-14; 75009 from 2016-09-06 to 2016-09-06; 75016 from 2016-09-15 to 2016-09-15; 75016 from 2016-09-15 to 2016-09-15; 75002 from 2016-10-03 to 2016-10-03; 75009 from 2016-09-06 to 2016-09-06; 75007 from 2016-10-10 to 2016-10-10; 75020 from 2016-09-14 to 2016-09-14; 75016 from 2016-10-13 to 2016-10-13; 75009 from 2016-09-29 to 2016-09-29; 75016 from 2016-10-10 to 2016-10-10; 75020 from 2016-09-30 to 2016-09-30; 75019 from 2016-09-16 to 2016-09-16; 75009 from 2016-09-06 to 2016-09-06; 75009 from 2016-09-29 to 2016-09-29; 75020 from 2016-09-30 to 2016-09-30; 75019 from 2016-10-03 to 2016-10-03; 75016 from 2016-09-15 to 2016-09-15; 75009 from 2016-09-29 to 2016-09-29; 75016 from 2016-09-15 to 2016-09-15; 75001 from 2016-09-05 to 2016-09-05; 75008 from 2016-10-11 to 2016-10-11; 75016 from 2016-09-15 to 2016-09-15; 75016 from 2016-10-13 to 2016-10-13; 75002 from 2016-09-19 to 2016-09-19; 75016 from 2016-09-15 to 2016-09-15; 75009 from 2016-09-29 to 2016-09-29; 75016 from 2016-10-05 to 2016-10-05; 75008 from 2016-09-13 to 2016-09-13; 75020 from 2016-09-30 to 2016-09-30; 75007 from 2016-10-10 to 2016-10-10; 75009 from 2016-09-29 to 2016-09-29; 75016 from 2016-10-13 to 2016-10-13; 75001 from 2016-09-05 to 2016-09-05; 75016 from 2016-10-13 to 2016-10-13; 75020 from 2016-09-30 to 2016-09-30; 75016 from 2016-09-15 to 2016-09-15; 75007 from 2016-10-10 to 2016-10-10; 75016 from 2016-10-06 to 2016-10-06; 75007 from 2016-10-10 to 2016-10-10; 75016 from 2016-09-15 to 2016-09-15; 75009 from 2016-09-29 to 2016-09-29; 75019 from 2016-09-16 to 2016-09-16; 75007 from 2016-10-10 to 2016-10-10; 75009 from 2016-09-06 to 2016-09-06; 75020 from 2016-09-30 to 2016-09-30; 75020 from 2016-09-30 to 2016-09-30; \n",
      "\"War Machine\", by David Michod. Shootings: 75001 from 2016-01-30 to 2016-01-30; 75009 from 2016-01-30 to 2016-01-30; 75001 from 2016-01-31 to 2016-01-31; 75009 from 2016-01-31 to 2016-01-31; 75001 from 2016-01-31 to 2016-01-31; 75009 from 2016-01-31 to 2016-01-31; 75008 from 2016-01-31 to 2016-01-31; 75001 from 2016-01-31 to 2016-01-31; 75007 from 2016-01-30 to 2016-01-30; \n",
      "\"Stella Est Amoureuse\", by Sylvie Verheyde. Shootings: 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75005 from 2021-02-24 to 2021-02-24; 75013 from 2021-02-23 to 2021-02-23; 75003 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-18 to 2021-02-18; 75013 from 2021-02-22 to 2021-02-22; 75013 from 2021-02-16 to 2021-02-16; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-23 to 2021-02-23; 75003 from 2020-12-22 to 2020-12-22; 75003 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75003 from 2021-02-11 to 2021-02-12; 75005 from 2021-02-08 to 2021-02-08; 75001 from 2021-02-15 to 2021-02-15; 75005 from 2021-02-24 to 2021-02-24; 75013 from 2021-02-25 to 2021-02-26; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75005 from 2020-12-22 to 2020-12-22; 75003 from 2021-02-11 to 2021-02-12; 75013 from 2021-02-16 to 2021-02-16; 75003 from 2021-02-09 to 2021-02-10; 75013 from 2021-02-23 to 2021-02-23; 75012 from 2021-02-25 to 2021-02-25; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75005 from 2021-02-08 to 2021-02-08; 75013 from 2021-02-16 to 2021-02-16; 75013 from 2021-02-23 to 2021-02-23; 75013 from 2021-02-23 to 2021-02-23; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-18 to 2021-02-18; 75005 from 2021-02-24 to 2021-02-24; 75005 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75005 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-16 to 2021-02-16; 75001 from 2021-02-26 to 2021-02-26; 75013 from 2020-12-22 to 2020-12-22; 75005 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75003 from 2021-02-10 to 2021-02-11; 75005 from 2021-02-08 to 2021-02-08; 75013 from 2021-02-16 to 2021-02-16; 75013 from 2021-02-22 to 2021-02-22; 75013 from 2021-02-23 to 2021-02-23; 75005 from 2021-02-24 to 2021-02-24; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-23 to 2021-02-23; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75003 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75005 from 2021-02-08 to 2021-02-08; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75003 from 2021-02-12 to 2021-02-13; 75013 from 2021-02-16 to 2021-02-16; 75001 from 2021-02-15 to 2021-02-15; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-23 to 2021-02-23; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75005 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-23 to 2021-02-23; 75013 from 2021-02-23 to 2021-02-23; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75005 from 2021-02-08 to 2021-02-08; 75001 from 2021-02-26 to 2021-02-26; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75013 from 2020-12-22 to 2020-12-22; 75001 from 2020-12-22 to 2020-12-22; 75013 from 2021-02-23 to 2021-02-23; \n",
      "\"J'Accuse\", by Roman Polanski. Shootings: 75015 from 2019-02-25 to 2019-02-25; 75002 from 2019-04-25 to 2019-04-25; 75009 from 2019-02-13 to 2019-02-15; 75009 from 2019-01-31 to 2019-02-01; 75009 from 2019-02-13 to 2019-02-15; 75007 from 2018-11-26 to 2018-11-26; 75015 from 2019-02-26 to 2019-02-27; 75007 from 2018-11-26 to 2018-11-26; 75015 from 2019-02-22 to 2019-02-22; 75001 from 2019-01-29 to 2019-01-29; 75007 from 2018-11-26 to 2018-11-26; 75016 from 2019-02-20 to 2019-02-21; 75006 from 2019-03-04 to 2019-03-04; 75015 from 2019-02-21 to 2019-02-22; \n",
      "\"Le Bazar De La Charité\", by Alexandre Laurent. Shootings: 75008 from 2019-03-14 to 2019-03-15; 75008 from 2019-03-18 to 2019-03-18; 75016 from 2019-02-18 to 2019-02-24; 75004 from 2019-03-07 to 2019-03-07; 75016 from 2019-02-23 to 2019-02-27; 75004 from 2019-03-04 to 2019-03-05; 75004 from 2019-03-08 to 2019-03-08; 75008 from 2019-03-19 to 2019-03-19; 75008 from 2019-03-14 to 2019-03-16; 75016 from 2019-02-11 to 2019-02-19; 75008 from 2019-03-15 to 2019-03-16; 75008 from 2019-03-11 to 2019-03-16; 75004 from 2019-03-06 to 2019-03-06; 75008 from 2019-03-11 to 2019-03-12; 75005 from 2019-03-05 to 2019-03-06; 75004 from 2019-03-07 to 2019-03-07; 75005 from 2019-03-04 to 2019-03-04; 75008 from 2019-03-13 to 2019-03-13; 75005 from 2019-03-05 to 2019-03-05; \n",
      "\"Les Heroiques\", by Maxime Roy. Shootings: 75010 from 2019-03-11 to 2019-03-12; \n",
      "\"Paris Brest\", by Philippe Lioret. Shootings: 75006 from 2019-03-15 to 2019-03-15; 75006 from 2019-03-15 to 2019-03-15; \n",
      "\"The Eddy\", by Damien Chazelle. Shootings: 75013 from 2019-05-28 to 2019-05-28; 75012 from 2019-05-28 to 2019-05-28; 75013 from 2019-05-15 to 2019-05-16; 75012 from 2019-05-28 to 2019-05-28; 75020 from 2019-10-11 to 2019-10-12; 75016 from 2019-05-24 to 2019-05-24; 75016 from 2019-09-02 to 2019-09-02; 75012 from 2019-06-17 to 2019-06-20; 75012 from 2019-08-21 to 2019-08-31; 75020 from 2019-08-20 to 2019-08-20; 75012 from 2019-10-03 to 2019-10-04; 75012 from 2019-09-13 to 2019-09-13; 75016 from 2019-05-24 to 2019-05-24; 75020 from 2019-05-16 to 2019-05-17; 75020 from 2019-09-30 to 2019-10-02; 75020 from 2019-08-14 to 2019-08-14; 75004 from 2019-08-19 to 2019-08-19; 75020 from 2019-04-11 to 2019-04-11; 75013 from 2019-05-28 to 2019-05-29; 75012 from 2019-05-31 to 2019-06-01; 75011 from 2019-08-20 to 2019-08-21; 75014 from 2019-05-22 to 2019-05-22; 75013 from 2019-05-28 to 2019-05-28; 75016 from 2019-08-09 to 2019-08-09; 75012 from 2019-06-11 to 2019-06-11; 75020 from 2019-05-07 to 2019-05-07; 75012 from 2019-09-18 to 2019-09-20; 75020 from 2019-09-30 to 2019-10-01; 75013 from 2019-05-28 to 2019-05-28; 75019 from 2019-05-07 to 2019-05-08; 75010 from 2019-08-20 to 2019-08-22; 75019 from 2019-06-24 to 2019-06-25; 75013 from 2019-05-27 to 2019-05-29; 75012 from 2019-05-28 to 2019-05-28; 75012 from 2019-06-04 to 2019-06-04; 75014 from 2019-08-06 to 2019-08-08; 75012 from 2019-07-15 to 2019-07-16; 75014 from 2019-05-23 to 2019-05-24; 75018 from 2019-06-28 to 2019-06-28; 75012 from 2019-06-05 to 2019-06-05; 75116 from 2019-09-02 to 2019-09-03; 75014 from 2019-07-16 to 2019-07-17; 75019 from 2019-08-20 to 2019-08-20; 75012 from 2019-09-17 to 2019-09-17; 75014 from 2019-05-23 to 2019-05-23; 75012 from 2019-09-12 to 2019-09-12; 75012 from 2019-09-17 to 2019-09-17; 75116 from 2019-09-24 to 2019-09-24; 75013 from 2019-05-27 to 2019-05-28; 75012 from 2019-05-30 to 2019-05-30; 75007 from 2019-04-30 to 2019-05-01; 75020 from 2019-05-07 to 2019-05-08; 75012 from 2019-05-29 to 2019-05-29; 75016 from 2019-09-24 to 2019-09-24; 75014 from 2019-05-22 to 2019-05-24; 75019 from 2019-04-11 to 2019-04-11; 75010 from 2019-07-04 to 2019-07-04; 75012 from 2019-06-26 to 2019-06-26; 75002 from 2019-07-01 to 2019-07-02; \n",
      "\"La Sage Femme\", by Martin Provost. Shootings: 75008 from 2016-04-11 to 2016-04-11; 75008 from 2016-04-05 to 2016-04-06; 75018 from 2016-03-31 to 2016-03-31; 75005 from 2016-04-13 to 2016-04-13; 75018 from 2016-05-24 to 2016-05-24; 75008 from 2016-04-11 to 2016-04-11; 75018 from 2016-03-31 to 2016-03-31; 75018 from 2016-05-24 to 2016-05-24; 75016 from 2016-05-20 to 2016-05-20; 75018 from 2016-05-24 to 2016-05-24; 75005 from 2016-04-12 to 2016-04-12; 75008 from 2016-04-11 to 2016-04-11; 75018 from 2016-05-24 to 2016-05-24; 75008 from 2016-04-11 to 2016-04-11; 75005 from 2016-04-13 to 2016-04-13; 75016 from 2016-05-04 to 2016-05-04; 75008 from 2016-04-07 to 2016-04-07; \n",
      "\"Modern Family 11\", by Jim Bagdonas. Shootings: 75005 from 2019-11-15 to 2019-11-15; 75004 from 2019-11-15 to 2019-11-15; 75004 from 2019-11-15 to 2019-11-15; 75005 from 2019-11-15 to 2019-11-15; 75004 from 2019-11-15 to 2019-11-15; \n",
      "\"Alice Nevers\", by Julien Zidi. Shootings: 75116 from 2019-11-19 to 2019-11-19; 75116 from 2019-11-21 to 2019-11-21; 75001 from 2019-11-16 to 2019-11-16; 75009 from 2018-11-20 to 2018-11-21; 75016 from 2018-04-12 to 2018-04-12; 75009 from 2017-01-23 to 2017-01-23; 75001 from 2017-10-16 to 2017-10-16; 75001 from 2017-10-09 to 2017-10-09; 75007 from 2018-11-26 to 2018-11-26; 75007 from 2017-01-19 to 2017-01-19; 75116 from 2019-11-22 to 2019-11-23; 75014 from 2017-10-18 to 2017-10-18; 75001 from 2018-04-19 to 2018-04-19; 75004 from 2018-04-19 to 2018-04-20; 75014 from 2017-02-06 to 2017-02-06; 75007 from 2019-11-25 to 2019-11-25; 75006 from 2018-11-19 to 2018-11-20; 75002 from 2017-10-16 to 2017-10-16; 75013 from 2017-02-06 to 2017-02-06; 75001 from 2017-02-04 to 2017-02-04; 75001 from 2017-10-09 to 2017-10-09; 75019 from 2019-11-29 to 2019-11-29; 75019 from 2019-11-28 to 2019-11-28; 75001 from 2017-01-19 to 2017-01-19; 75013 from 2017-10-19 to 2017-10-19; 75014 from 2017-10-18 to 2017-10-18; 75011 from 2018-04-06 to 2018-04-07; 75116 from 2019-11-18 to 2019-11-21; 75116 from 2018-11-16 to 2018-11-16; 75019 from 2019-11-29 to 2019-11-29; 75013 from 2017-10-18 to 2017-10-19; 75001 from 2019-11-13 to 2019-11-13; 75007 from 2018-11-26 to 2018-11-26; 75014 from 2017-10-18 to 2017-10-19; 75019 from 2017-01-26 to 2017-01-27; 75001 from 2019-11-16 to 2019-11-17; 75001 from 2018-04-19 to 2018-04-19; 75018 from 2017-01-25 to 2017-01-25; 75014 from 2017-10-18 to 2017-10-18; 75001 from 2019-11-16 to 2019-11-16; 75016 from 2018-04-12 to 2018-04-12; 75019 from 2019-11-28 to 2019-11-28; 75009 from 2018-04-04 to 2018-04-05; \n",
      "\"Forte\", by Katia Lewkowicz. Shootings: 75018 from 2018-12-07 to 2018-12-08; 75019 from 2018-12-05 to 2018-12-06; 75018 from 2018-11-08 to 2018-11-08; 75008 from 2018-11-13 to 2018-11-13; 75009 from 2018-12-20 to 2018-12-21; 75019 from 2018-11-08 to 2018-11-08; 75008 from 2018-11-09 to 2018-11-16; 75018 from 2018-12-07 to 2018-12-07; 75001 from 2018-11-28 to 2018-11-28; 75020 from 2018-11-29 to 2018-11-29; 75008 from 2018-11-16 to 2018-11-16; 75009 from 2018-11-27 to 2018-11-27; 75001 from 2018-11-28 to 2018-11-28; 75020 from 2018-12-20 to 2018-12-20; 75019 from 2018-11-20 to 2018-11-26; 75018 from 2018-12-10 to 2018-12-10; 75018 from 2018-12-11 to 2018-12-11; 75008 from 2018-11-14 to 2018-11-14; 75019 from 2018-11-22 to 2018-11-22; 75116 from 2018-12-06 to 2018-12-08; 75001 from 2018-11-28 to 2018-11-28; 75116 from 2018-12-06 to 2018-12-08; 75018 from 2018-12-13 to 2018-12-20; 75013 from 2018-12-03 to 2018-12-04; 75018 from 2018-12-10 to 2018-12-12; 75020 from 2018-11-30 to 2018-11-30; 75020 from 2018-11-30 to 2018-11-30; 75019 from 2018-12-05 to 2018-12-05; 75020 from 2018-11-29 to 2018-11-29; 75020 from 2018-11-29 to 2018-11-29; 75009 from 2018-12-20 to 2018-12-20; 75008 from 2018-11-09 to 2018-11-09; 75020 from 2018-12-20 to 2018-12-20; \n"
     ]
    }
   ],
   "source": [
    "movies= list(movies.items())\n",
    "   \n",
    "all_movie_displays = [display_movie(m) for m in movies]\n",
    "print('\\n'.join(all_movie_displays[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd61327",
   "metadata": {},
   "source": [
    "- Display for each district its number of shootings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db17e30",
   "metadata": {},
   "source": [
    "We created an empty dictionary stats which will have keys as each distinct district present in our movies dictionary, and values as the number of times this district number appears in our movies dictionary , which is basically the number of shootings that happened in that district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87acd46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'75012': 596,\n",
       " '75020': 587,\n",
       " '75011': 641,\n",
       " '75019': 745,\n",
       " '75018': 1043,\n",
       " '75005': 640,\n",
       " '75009': 642,\n",
       " '75001': 722,\n",
       " '75004': 670,\n",
       " '75010': 749,\n",
       " '75007': 657,\n",
       " '75003': 236,\n",
       " '75017': 378,\n",
       " '75013': 658,\n",
       " '75008': 798,\n",
       " '75015': 363,\n",
       " '75002': 297,\n",
       " '75006': 471,\n",
       " '75116': 421,\n",
       " '75016': 614,\n",
       " '75014': 321,\n",
       " '94320': 4,\n",
       " '[Unknown]': 1,\n",
       " '93500': 6,\n",
       " '93320': 1,\n",
       " '92220': 1,\n",
       " '92170': 1,\n",
       " '93200': 1,\n",
       " '93000': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats=dict()\n",
    "\n",
    "for value in movies:\n",
    "    # we loop the shooting_locations array of each movie, and get the district of each element. \n",
    "    # if the district appears in stats, we just want to increment its value by 1\n",
    "    for loc in value[1]['shooting_locations']:\n",
    "        district=loc[\"district\"]\n",
    "        if exists(stats, district):\n",
    "            stats[district]=stats[district]+1\n",
    "    # if the district is not in our dictionary stats, we add it along with the value 1, meaning so far it only occurred once in our movies dictionary\n",
    "        else:\n",
    "            stats[district] = 1\n",
    "\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770ae9d",
   "metadata": {},
   "source": [
    "# Exercice 4 - Analyze CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd13022",
   "metadata": {},
   "source": [
    "- Write a Python code retrieves the file of the most loaned titles in libraries in Paris at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a3e8780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:35.818501Z",
     "start_time": "2024-10-11T07:03:35.808870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type de document;Prêts 2022;Titre;Auteur;Nombre de localisations;Nombre de prêt total;Nombre d'exemplaires\r\n",
      "Bande dessinée jeunesse;1064;Razzia;Sobral,  Patrick;47;2938;67\r\n",
      "Bande dessinée jeunesse;1024;Touche pas à mon veau;Guibert,  Emmanuel;45;2296;71\r\n",
      "Bande dessinée jeunesse;1016;Max et Lili vont chez papy et mamie;Saint-Mars,  Dominique de;50;5554;103\r\n",
      "Bande dessinée jeunesse;938;Lili veut un petit chat;Saint-Mars,  Dominique de;51;5789;80\r\n",
      "Bande dessinée jeunesse;921;Max et Lili font du cam\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from requests import Session\n",
    "from io import StringIO\n",
    "\n",
    "s= Session()\n",
    "url = \"https://opendata.paris.fr/explore/dataset/les-titres-les-plus-pretes/download/?format=csv&timezone=Europe/Berlin&lang=en&use_labels_for_header=true&csv_separator=%3B\"\n",
    "\n",
    "books_loaned = s.get(url).text\n",
    "print(books_loaned[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3421c",
   "metadata": {},
   "source": [
    "We can note that the CSV file is separated by a \";\" between each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3a6a2",
   "metadata": {},
   "source": [
    "- Analyze the resulting CSV file to display, for all entries: title, author, and total number of loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d440b",
   "metadata": {},
   "source": [
    "The CSV file contains in each line, a document or a title in a library. Each column is an information about the type of the document, the number of loans in 2022, the title of the document, the authors, the number of locations, the total number of loans, and the number of copies.\n",
    "\n",
    "To display the titles in the specific format title, author, and total number of loans, we would need from each line of the CSV file the following fields: Titre, Auteur, and Nombre de prêt total. \n",
    "\n",
    "We opened the csv file for processing using the StringIO method, read the csv file while specifying \";\" as the delimiter, and transformed it into a list of array to process it more easily. The fields we want to access are at the following indeces: 2,3, and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce97337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.078350Z",
     "start_time": "2024-10-11T07:03:36.075015Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def disp_book(book):\n",
    "    title=book[2]\n",
    "    author=book[3]\n",
    "    loans=book[5]\n",
    "    s=\"\\\"\"+title+\"\\\", by \"+author+\" (\"+str(loans)+\" loans)\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f122f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Razzia\", by Sobral,  Patrick (2938 loans)\n",
      "\"Touche pas à mon veau\", by Guibert,  Emmanuel (2296 loans)\n",
      "\"Max et Lili vont chez papy et mamie\", by Saint-Mars,  Dominique de (5554 loans)\n",
      "\"Lili veut un petit chat\", by Saint-Mars,  Dominique de (5789 loans)\n",
      "\"Max et Lili font du camping\", by Saint-Mars,  Dominique de (5658 loans)\n",
      "\"Lili trouve sa maîtresse méchante\", by Saint-Mars,  Dominique de (4694 loans)\n",
      "\"J'irai où tu iras\", by Lyfoung,  Patricia (4707 loans)\n",
      "\"Les nerfs à vif\", by Nob (2837 loans)\n",
      "\"Je crois que je t'aime\", by Lyfoung,  Patricia (3878 loans)\n",
      "\"Attention tornade\", by Cazenove,  Christophe (2366 loans)\n",
      "\"Max et Lili se posent des questions sur Dieu\", by Saint-Mars,  Dominique de (4823 loans)\n",
      "\"Game over. 13. Toxic affair\", by Midam (2652 loans)\n",
      "\"Les Schtroumpfs et la tempête blanche\", by Jost,  Alain (975 loans)\n",
      "\"On a marché sur la lune\", by Hergé (5674 loans)\n",
      "\"Astérix chez les Bretons\", by Goscinny,  René (3014 loans)\n",
      "\"Parvati\", by Ogaki,  Philippe (2616 loans)\n",
      "\"Les Schtroumpfs et l'arbre d'or\", by Culliford,  Thierry (3460 loans)\n",
      "\"La décision : roman\", by Tuil,  Karine (976 loans)\n",
      "\"Les cahiers d'Esther. 4. Histoires de mes 13 ans\", by Sattouf,  Riad (2171 loans)\n",
      "\"Salut, les zinzins !\", by Cohen,  Jacqueline (4565 loans)\n"
     ]
    }
   ],
   "source": [
    "with StringIO(books_loaned) as csvfile:\n",
    "    r = csv.reader(csvfile, delimiter=';')\n",
    "    books = list(r) # transforming r to a list of arrays. (array of arrays because in the next code provided, we should loop firs 20 rows of books only)\n",
    "    books=books[1:] # removing the first line which is the header of the csv file from the list of arrays and keeping all the other entries of the documents.\n",
    "    print('\\n'.join( [disp_book(b) for b in books[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53915548",
   "metadata": {},
   "source": [
    "- Display for each type of document (there can be several entries for the same type of document), the total number of loans for this type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060e121",
   "metadata": {},
   "source": [
    "For this part, we created a stats dictionary that has for key values each type of document we have in the CSV file. The different document types can be accessed in the books list of arrays that we created at index 0 of each array. After obtaining the type, we also store the total number of loans in a variable loans. \n",
    "We add the document type to our stats dictionary with a value = total number of loans. If the type already exists in our dictionary, we edit its value to = its current value + total number of loans of the current document array we're looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6564071b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bande dessinée jeunesse': 2300143,\n",
       " 'Bande dessinée ado': 29819,\n",
       " 'Livre jeunesse': 104067,\n",
       " 'Bande dessinée adulte': 59726,\n",
       " 'Livre adulte': 41731,\n",
       " 'Jeux vidéos tous publics Non prêtables': 4235,\n",
       " 'Jeux de société prêtable': 10057,\n",
       " 'Livre sonore jeunesse': 10630,\n",
       " 'DVD jeunesse': 2471,\n",
       " 'Musique jeunesse': 4792,\n",
       " 'Jeux de société': 1753}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats=dict()\n",
    "with StringIO(books_loaned) as csvfile:\n",
    "    r = csv.reader(csvfile, delimiter=';')\n",
    "    books = list(r) # transforming r to a list of arrays. (array of arrays because in the next code provided, we should loop firs 20 rows of books only)\n",
    "    books=books[1:] # removing the header elt from the list of arrays\n",
    "    for book in books:\n",
    "        type_doc=book[0] # the type is obtained from the array book\n",
    "        loans=int(book[5]) # the total number of loans of the document\n",
    "        if exists(stats, type_doc):\n",
    "            stats[type_doc]=stats[type_doc]+loans # if the type exists in stats, edit its value with its current value+total number of loans of the current document we're looping \n",
    "        else:\n",
    "            stats[type_doc] = loans # if the type does not exist in stats, add it with a value equals to the total number of loans of the current document we're looping\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a5a1e",
   "metadata": {},
   "source": [
    "- Display titles in order of profitability (in descending order of the number of loans per copy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ef4ac",
   "metadata": {},
   "source": [
    "In this question, We noticed that in the answer, the display contains the number of copies as well, which was not in the display_book function we created above. For that reason, we modified it to include the number of copies as well.\n",
    "\n",
    "The format now is: \"title of document\", by author (total number of loans, number of copies)\n",
    "\n",
    "We also noted that for some documents, the author name is not available, so we replaced it by [Unknown] in that case.\n",
    "\n",
    "For the sorting, we used the sorted() function and gave it a key lambda of x[5]/x[6] which is basically the total number of loans divided by the number of copies, and we sorted according to the result. The sorting was in a descending order, which is why we added the reverse=True parameter.\n",
    "\n",
    "Source: <br> https://www.w3schools.com/python/ref_func_sorted.asp <br> https://www.freecodecamp.org/news/lambda-sort-list-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e64f69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying disp_book to display the copies number as well:\n",
    "def disp_book(book):\n",
    "    title=book[2]\n",
    "    author=book[3]\n",
    "    loans=book[5]\n",
    "    copies=book[6]\n",
    "    if author==\"\":\n",
    "        author=\"[Unknown]\"\n",
    "    s=\"\\\"\"+title+\"\\\", by \"+author+\" (\"+str(loans)+\" loans, \"+str(copies)+\" copies)\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70ea1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Console Nintendo Switch\", by [Unknown] (1648 loans, 2 copies)\n",
      "\"Console PlayStation 4\", by [Unknown] (2587 loans, 6 copies)\n",
      "\"SOS ouistiti :\", by [Unknown] (1868 loans, 5 copies)\n",
      "\"Quatre en ligne :\", by [Unknown] (1753 loans, 5 copies)\n",
      "\"Perplexus : : original\", by [Unknown] (2254 loans, 8 copies)\n",
      "\"Un enfant chez les schtroumpfs\", by Díaz Vizoso,  Miguel (4504 loans, 43 copies)\n",
      "\"Mon meilleur ami\", by Verron,  Laurent (4662 loans, 47 copies)\n",
      "\"Les vacances infernales\", by Cohen,  Jacqueline (5014 loans, 51 copies)\n",
      "\"Bande de sauvages !\", by Cohen,  Jacqueline (5761 loans, 60 copies)\n",
      "\"Trop, c'est trop !\", by Cohen,  Jacqueline (4504 loans, 47 copies)\n",
      "\"Les fous du mercredi\", by Cohen,  Jacqueline (5169 loans, 54 copies)\n",
      "\"Ca va chauffer !\", by Cohen,  Jacqueline (4071 loans, 44 copies)\n",
      "\"Uno :\", by [Unknown] (3136 loans, 34 copies)\n",
      "\"Ca roule !\", by Cohen,  Jacqueline (5763 loans, 63 copies)\n",
      "\"Salut, les zinzins !\", by Cohen,  Jacqueline (4565 loans, 50 copies)\n",
      "\"Les deux terreurs\", by Cohen,  Jacqueline (3999 loans, 44 copies)\n",
      "\"Subliiiimes !\", by Cohen,  Jacqueline (5007 loans, 56 copies)\n",
      "\"Un copieur sachant copier\", by Godi,  Bernard (3481 loans, 39 copies)\n",
      "\"A l'attaque !\", by Cohen,  Jacqueline (4353 loans, 49 copies)\n",
      "\"Tom-Tom et l'impossible Nana\", by Cohen,  Jacqueline (5832 loans, 66 copies)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "with StringIO(books_loaned) as csvfile:\n",
    "    r = csv.reader(csvfile, delimiter=';')\n",
    "    books = list(r)\n",
    "    books=books[1:]\n",
    "    sorted_books=sorted(books, key= lambda x: float(x[5]) / float(x[6]), reverse=True)\n",
    "    print('\\n'.join( [disp_book(b) for b in sorted_books[:20]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99791153",
   "metadata": {},
   "source": [
    "# Exercice 5 * - Analyze HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4ae10",
   "metadata": {},
   "source": [
    "- Write a Python program that gets the content of the Wikipedia page at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c676cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.180211Z",
     "start_time": "2024-10-11T07:03:36.168632Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get('https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_density')\n",
    "#print(page.text)\n",
    "\n",
    "#we used the same system than we had at excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20355320",
   "metadata": {},
   "source": [
    "- Display all the countries mentioned in the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28063da",
   "metadata": {},
   "source": [
    "In this code we are using beautifulsoup (mostly because trying to do this with regular expressions is very time consuming) \n",
    "\n",
    "First, we get the information from the page and then we get the wikitable to have the information of the countries, then we create a list and create a for loop for every cell in the table, then we check if the cell is not empty, and if it isn't we take the information from the first column and add it to the list.\n",
    "\n",
    "At the end, we print every element of countries <br>\n",
    "\n",
    "Source: <br>\n",
    "https://www.datahen.com/blog/web-scraping-using-python-beautiful-soup/ <br>\n",
    "https://stackoverflow.com/questions/69940519/web-scraping-wikipedia-with-beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c3d9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macau (China)\n",
      "Monaco\n",
      "Singapore\n",
      "Hong Kong (China)\n",
      "Gibraltar (UK)\n",
      "Bahrain\n",
      "Maldives\n",
      "Malta\n",
      "Vatican City\n",
      "Sint Maarten (NL)\n",
      "Bermuda (UK)\n",
      "Bangladesh\n",
      "Guernsey (UK)\n",
      "Jersey (UK)\n",
      "Mayotte (France)\n",
      "Palestine(Gaza Strip, West Bank)\n",
      "Taiwan\n",
      "Mauritius\n",
      "Barbados\n",
      "Nauru\n",
      "Saint Martin (France)\n",
      "Aruba (NL)\n",
      "San Marino\n",
      "Rwanda\n",
      "South Korea\n",
      "Lebanon\n",
      "Saint Barthélemy (France)\n",
      "Burundi\n",
      "Tuvalu\n",
      "India\n",
      "Curaçao (NL)\n",
      "Netherlands\n",
      "Haiti\n",
      "Israel\n",
      "Réunion (France)\n",
      "Philippines\n",
      "Belgium\n",
      "Comoros\n",
      "Grenada\n",
      "Puerto Rico (US)\n",
      "Martinique (France)\n",
      "Sri Lanka\n",
      "Japan\n",
      "Guam (US)\n",
      "El Salvador\n",
      "Pakistan\n",
      "Trinidad and Tobago\n",
      "Vietnam\n",
      "Saint Lucia\n",
      "U.S. Virgin Islands (US)\n",
      "United Kingdom\n",
      "Saint Vincent and the Grenadines\n",
      "Cayman Islands (UK)\n",
      "Jamaica\n",
      "Luxembourg\n",
      "Liechtenstein\n",
      "Gambia\n",
      "Nigeria\n",
      "Kuwait\n",
      "Guadeloupe (France)\n",
      "São Tomé and Príncipe\n",
      "Seychelles\n",
      "Qatar\n",
      "Germany\n",
      "Dominican Republic\n",
      "Marshall Islands\n",
      "Malawi\n",
      "American Samoa (US)\n",
      "North Korea\n",
      "Antigua and Barbuda\n",
      "Switzerland\n",
      "Nepal\n",
      "British Virgin Islands (UK)\n",
      "Uganda\n",
      "Italy\n",
      "Kiribati\n",
      "Saint Kitts and Nevis\n",
      "Anguilla (UK)\n",
      "Andorra\n",
      "Guatemala\n",
      "Micronesia\n",
      "Togo\n",
      "Tokelau (New Zealand)\n",
      "Kosovo\n",
      "China\n",
      "Cape Verde\n",
      "Isle of Man\n",
      "Indonesia\n",
      "Tonga\n",
      "Ghana\n",
      "Thailand\n",
      "Denmark\n",
      "Cyprus\n",
      "United Arab Emirates\n",
      "Transnistria\n",
      "Czech Republic\n",
      "Jordan\n",
      "Syria\n",
      "Sierra Leone\n",
      "Poland\n",
      "Azerbaijan\n",
      "Benin\n",
      "Slovakia\n",
      "France (metropolitan)\n",
      "Ethiopia\n",
      "Northern Cyprus\n",
      "Egypt\n",
      "Portugal\n",
      "Turkey\n",
      "Hungary\n",
      "Northern Mariana Islands (US)\n",
      "Austria\n",
      "Iraq\n",
      "Slovenia\n",
      "Malaysia\n",
      "Costa Rica\n",
      "Cuba\n",
      "Moldova\n",
      "France (metropolitan & overseas)\n",
      "Albania\n",
      "Dominica\n",
      "Spain\n",
      "Honduras\n",
      "Cambodia\n",
      "Armenia\n",
      "Kenya\n",
      "East Timor\n",
      "Senegal\n",
      "Ivory Coast\n",
      "Burkina Faso\n",
      "French Polynesia (France)\n",
      "Romania\n",
      "Caribbean Netherlands (Netherlands)\n",
      "North Macedonia\n",
      "Wallis and Futuna (France)\n",
      "Serbia\n",
      "Myanmar\n",
      "Samoa\n",
      "Brunei\n",
      "Greece\n",
      "Uzbekistan\n",
      "Lesotho\n",
      "Tunisia\n",
      "Ireland\n",
      "Cook Islands\n",
      "Tajikistan\n",
      "Tanzania\n",
      "Croatia\n",
      "Ecuador\n",
      "Eswatini\n",
      "Mexico\n",
      "Yemen\n",
      "Afghanistan\n",
      "Bosnia and Herzegovina\n",
      "Akrotiri and Dhekelia (UK)\n",
      "Equatorial Guinea\n",
      "Ukraine\n",
      "Bulgaria\n",
      "Cameroon\n",
      "World (excluding Antarctica)\n",
      "Guinea-Bissau\n",
      "Panama\n",
      "Guinea\n",
      "Norfolk Island (Australia)\n",
      "Iran\n",
      "Nicaragua\n",
      "World (all land)\n",
      "Georgia\n",
      "Morocco\n",
      "Madagascar\n",
      "Fiji\n",
      "South Africa\n",
      "Djibouti\n",
      "Liberia\n",
      "Turks and Caicos Islands (UK)\n",
      "Easter Island\n",
      "Belarus\n",
      "Colombia\n",
      "Montenegro\n",
      "DR Congo\n",
      "Saint Helena, Ascension and Tristan da Cunha (UK)\n",
      "Cocos (Keeling) Islands (Australia)\n",
      "Zimbabwe\n",
      "Montserrat (UK)\n",
      "Mozambique\n",
      "Lithuania\n",
      "Palau\n",
      "Faroe Islands (Denmark)\n",
      "United States\n",
      "Kyrgyzstan\n",
      "Laos\n",
      "Venezuela\n",
      "Eritrea\n",
      "Bahamas\n",
      "Angola\n",
      "Somaliland\n",
      "Estonia\n",
      "Somalia\n",
      "Latvia\n",
      "Abkhazia\n",
      "Vanuatu\n",
      "Zambia\n",
      "Sudan\n",
      "Peru\n",
      "Chile\n",
      "Solomon Islands\n",
      "Brazil\n",
      "Sweden\n",
      "Saint Pierre and Miquelon (France)\n",
      "Papua New Guinea\n",
      "Niger\n",
      "Bhutan\n",
      "Uruguay\n",
      "New Zealand\n",
      "Algeria\n",
      "Åland (Finland)\n",
      "Mali\n",
      "Belize\n",
      "Congo\n",
      "South Sudan\n",
      "Saudi Arabia\n",
      "Finland\n",
      "Argentina\n",
      "South Ossetia\n",
      "New Caledonia (France)\n",
      "Paraguay\n",
      "Oman\n",
      "Chad\n",
      "Norway\n",
      "Turkmenistan\n",
      "Christmas Island (Australia)\n",
      "Bolivia\n",
      "Central African Republic\n",
      "Gabon\n",
      "Russia\n",
      "Niue\n",
      "Kazakhstan\n",
      "Mauritania\n",
      "Botswana\n",
      "Libya\n",
      "Canada\n",
      "Suriname\n",
      "Guyana\n",
      "French Guiana (France)\n",
      "Iceland\n",
      "Australia\n",
      "Namibia\n",
      "Western Sahara (disputed)\n",
      "Mongolia\n",
      "Pitcairn Islands (UK)\n",
      "Falkland Islands (UK)\n",
      "Greenland (Denmark)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_density'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "table = soup.find('table', class_='wikitable')\n",
    "\n",
    "countries = []\n",
    "for row in table.find_all('tr'):\n",
    "    columns = row.find_all('td')\n",
    "    if columns:\n",
    "        country_name = columns[0].text.strip()\n",
    "        countries.append(country_name)\n",
    "\n",
    "for country in countries:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37750247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:36.660044Z",
     "start_time": "2024-10-11T07:03:36.649884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Macau (China)',\n",
       " 'Monaco',\n",
       " 'Singapore',\n",
       " 'Hong Kong (China)',\n",
       " 'Gibraltar (UK)',\n",
       " 'Bahrain',\n",
       " 'Maldives',\n",
       " 'Malta',\n",
       " 'Vatican City',\n",
       " 'Sint Maarten (NL)',\n",
       " 'Bermuda (UK)',\n",
       " 'Bangladesh',\n",
       " 'Guernsey (UK)',\n",
       " 'Jersey (UK)',\n",
       " 'Mayotte (France)',\n",
       " 'Palestine(Gaza Strip, West Bank)',\n",
       " 'Taiwan',\n",
       " 'Mauritius',\n",
       " 'Barbados',\n",
       " 'Nauru',\n",
       " 'Saint Martin (France)',\n",
       " 'Aruba (NL)',\n",
       " 'San Marino',\n",
       " 'Rwanda',\n",
       " 'South Korea',\n",
       " 'Lebanon',\n",
       " 'Saint Barthélemy (France)',\n",
       " 'Burundi',\n",
       " 'Tuvalu',\n",
       " 'India',\n",
       " 'Curaçao (NL)',\n",
       " 'Netherlands',\n",
       " 'Haiti',\n",
       " 'Israel',\n",
       " 'Réunion (France)',\n",
       " 'Philippines',\n",
       " 'Belgium',\n",
       " 'Comoros',\n",
       " 'Grenada',\n",
       " 'Puerto Rico (US)',\n",
       " 'Martinique (France)',\n",
       " 'Sri Lanka',\n",
       " 'Japan',\n",
       " 'Guam (US)',\n",
       " 'El Salvador',\n",
       " 'Pakistan',\n",
       " 'Trinidad and Tobago',\n",
       " 'Vietnam',\n",
       " 'Saint Lucia',\n",
       " 'U.S. Virgin Islands (US)',\n",
       " 'United Kingdom',\n",
       " 'Saint Vincent and the Grenadines',\n",
       " 'Cayman Islands (UK)',\n",
       " 'Jamaica',\n",
       " 'Luxembourg',\n",
       " 'Liechtenstein',\n",
       " 'Gambia',\n",
       " 'Nigeria',\n",
       " 'Kuwait',\n",
       " 'Guadeloupe (France)',\n",
       " 'São Tomé and Príncipe',\n",
       " 'Seychelles',\n",
       " 'Qatar',\n",
       " 'Germany',\n",
       " 'Dominican Republic',\n",
       " 'Marshall Islands',\n",
       " 'Malawi',\n",
       " 'American Samoa (US)',\n",
       " 'North Korea',\n",
       " 'Antigua and Barbuda',\n",
       " 'Switzerland',\n",
       " 'Nepal',\n",
       " 'British Virgin Islands (UK)',\n",
       " 'Uganda',\n",
       " 'Italy',\n",
       " 'Kiribati',\n",
       " 'Saint Kitts and Nevis',\n",
       " 'Anguilla (UK)',\n",
       " 'Andorra',\n",
       " 'Guatemala',\n",
       " 'Micronesia',\n",
       " 'Togo',\n",
       " 'Tokelau (New Zealand)',\n",
       " 'Kosovo',\n",
       " 'China',\n",
       " 'Cape Verde',\n",
       " 'Isle of Man',\n",
       " 'Indonesia',\n",
       " 'Tonga',\n",
       " 'Ghana',\n",
       " 'Thailand',\n",
       " 'Denmark',\n",
       " 'Cyprus',\n",
       " 'United Arab Emirates',\n",
       " 'Transnistria',\n",
       " 'Czech Republic',\n",
       " 'Jordan',\n",
       " 'Syria',\n",
       " 'Sierra Leone',\n",
       " 'Poland',\n",
       " 'Azerbaijan',\n",
       " 'Benin',\n",
       " 'Slovakia',\n",
       " 'France (metropolitan)',\n",
       " 'Ethiopia',\n",
       " 'Northern Cyprus',\n",
       " 'Egypt',\n",
       " 'Portugal',\n",
       " 'Turkey',\n",
       " 'Hungary',\n",
       " 'Northern Mariana Islands (US)',\n",
       " 'Austria',\n",
       " 'Iraq',\n",
       " 'Slovenia',\n",
       " 'Malaysia',\n",
       " 'Costa Rica',\n",
       " 'Cuba',\n",
       " 'Moldova',\n",
       " 'France (metropolitan & overseas)',\n",
       " 'Albania',\n",
       " 'Dominica',\n",
       " 'Spain',\n",
       " 'Honduras',\n",
       " 'Cambodia',\n",
       " 'Armenia',\n",
       " 'Kenya',\n",
       " 'East Timor',\n",
       " 'Senegal',\n",
       " 'Ivory Coast',\n",
       " 'Burkina Faso',\n",
       " 'French Polynesia (France)',\n",
       " 'Romania',\n",
       " 'Caribbean Netherlands (Netherlands)',\n",
       " 'North Macedonia',\n",
       " 'Wallis and Futuna (France)',\n",
       " 'Serbia',\n",
       " 'Myanmar',\n",
       " 'Samoa',\n",
       " 'Brunei',\n",
       " 'Greece',\n",
       " 'Uzbekistan',\n",
       " 'Lesotho',\n",
       " 'Tunisia',\n",
       " 'Ireland',\n",
       " 'Cook Islands',\n",
       " 'Tajikistan',\n",
       " 'Tanzania',\n",
       " 'Croatia',\n",
       " 'Ecuador',\n",
       " 'Eswatini',\n",
       " 'Mexico',\n",
       " 'Yemen',\n",
       " 'Afghanistan',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Akrotiri and Dhekelia (UK)',\n",
       " 'Equatorial Guinea',\n",
       " 'Ukraine',\n",
       " 'Bulgaria',\n",
       " 'Cameroon',\n",
       " 'World (excluding Antarctica)',\n",
       " 'Guinea-Bissau',\n",
       " 'Panama',\n",
       " 'Guinea',\n",
       " 'Norfolk Island (Australia)',\n",
       " 'Iran',\n",
       " 'Nicaragua',\n",
       " 'World (all land)',\n",
       " 'Georgia',\n",
       " 'Morocco',\n",
       " 'Madagascar',\n",
       " 'Fiji',\n",
       " 'South Africa',\n",
       " 'Djibouti',\n",
       " 'Liberia',\n",
       " 'Turks and Caicos Islands (UK)',\n",
       " 'Easter Island',\n",
       " 'Belarus',\n",
       " 'Colombia',\n",
       " 'Montenegro',\n",
       " 'DR Congo',\n",
       " 'Saint Helena, Ascension and Tristan da Cunha (UK)',\n",
       " 'Cocos (Keeling) Islands (Australia)',\n",
       " 'Zimbabwe',\n",
       " 'Montserrat (UK)',\n",
       " 'Mozambique',\n",
       " 'Lithuania',\n",
       " 'Palau',\n",
       " 'Faroe Islands (Denmark)',\n",
       " 'United States',\n",
       " 'Kyrgyzstan',\n",
       " 'Laos',\n",
       " 'Venezuela',\n",
       " 'Eritrea',\n",
       " 'Bahamas',\n",
       " 'Angola',\n",
       " 'Somaliland',\n",
       " 'Estonia',\n",
       " 'Somalia',\n",
       " 'Latvia',\n",
       " 'Abkhazia',\n",
       " 'Vanuatu',\n",
       " 'Zambia',\n",
       " 'Sudan',\n",
       " 'Peru',\n",
       " 'Chile',\n",
       " 'Solomon Islands',\n",
       " 'Brazil',\n",
       " 'Sweden',\n",
       " 'Saint Pierre and Miquelon (France)',\n",
       " 'Papua New Guinea',\n",
       " 'Niger',\n",
       " 'Bhutan',\n",
       " 'Uruguay',\n",
       " 'New Zealand',\n",
       " 'Algeria',\n",
       " 'Åland (Finland)',\n",
       " 'Mali',\n",
       " 'Belize',\n",
       " 'Congo',\n",
       " 'South Sudan',\n",
       " 'Saudi Arabia',\n",
       " 'Finland',\n",
       " 'Argentina',\n",
       " 'South Ossetia',\n",
       " 'New Caledonia (France)',\n",
       " 'Paraguay',\n",
       " 'Oman',\n",
       " 'Chad',\n",
       " 'Norway',\n",
       " 'Turkmenistan',\n",
       " 'Christmas Island (Australia)',\n",
       " 'Bolivia',\n",
       " 'Central African Republic',\n",
       " 'Gabon',\n",
       " 'Russia',\n",
       " 'Niue',\n",
       " 'Kazakhstan',\n",
       " 'Mauritania',\n",
       " 'Botswana',\n",
       " 'Libya',\n",
       " 'Canada',\n",
       " 'Suriname',\n",
       " 'Guyana',\n",
       " 'French Guiana (France)',\n",
       " 'Iceland',\n",
       " 'Australia',\n",
       " 'Namibia',\n",
       " 'Western Sahara (disputed)',\n",
       " 'Mongolia',\n",
       " 'Pitcairn Islands (UK)',\n",
       " 'Falkland Islands (UK)',\n",
       " 'Greenland (Denmark)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dd356",
   "metadata": {},
   "source": [
    "- Display for each country its rank, density, population, area. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15e80c",
   "metadata": {},
   "source": [
    "In this case we make an operation very similar to the one above, but this time we create more lists to save each information. \n",
    "\n",
    "Here, we do not create a list for the rank, this is because we are taking the elements in order so they are already ordered.\n",
    "\n",
    "At the end, we just print every information for each country but we add the i counter as the rank (this because the countries are ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d116316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macau (China) 1 , 21,000, 704,150, 33\n",
      "Monaco 2 , 18,000, 36,298, 2.0\n",
      "Singapore 3 , 8,250, 6,014,723, 729\n",
      "Hong Kong (China) 4 , 6,725, 7,491,609, 1,114\n",
      "Gibraltar (UK) 5 , 4,800, 32,688, 6.8\n",
      "Bahrain 6 , 1,910, 1,485,510, 778\n",
      "Maldives 7 , 1,750, 523,787, 300\n",
      "Malta 8 , 1,700, 535,065, 315\n",
      "Vatican City 9 , 1,600, 764, 0.49\n",
      "Sint Maarten (NL) 10 , 1,300, 44,222, 34\n",
      "Bermuda (UK) 11 , 1,200, 64,069, 54\n",
      "Bangladesh 12 , 1,165, 172,954,319, 148,460\n",
      "Guernsey (UK) 13 , 990, 63,544, 64\n",
      "Jersey (UK) 14 , 964, 111,803, 116\n",
      "Mayotte (France) 15 , 913, 335,995, 368\n",
      "Palestine(Gaza Strip, West Bank) 16 , 892, 5,371,230, 6,020\n",
      "Taiwan 17 , 676, 23,923,277, 35,410\n",
      "Mauritius 18 , 657, 1,300,557, 1,979\n",
      "Barbados 19 , 654, 281,996, 431\n",
      "Nauru 20 , 610, 12,780, 21\n",
      "Saint Martin (France) 21 , 610, 32,077, 53\n",
      "Aruba (NL) 22 , 590, 106,277, 180\n",
      "San Marino 23 , 550, 33,642, 61\n",
      "Rwanda 24 , 535, 14,094,683, 26,338\n",
      "South Korea 25 , 516, 51,784,059, 100,413\n",
      "Lebanon 26 , 512, 5,353,930, 10,452\n",
      "Saint Barthélemy (France) 27 , 500, 10,994, 22\n",
      "Burundi 28 , 476, 13,238,559, 27,834\n",
      "Tuvalu 29 , 440, 11,396, 26\n",
      "India 30 , 435, 1,428,627,663, 3,287,263\n",
      "Curaçao (NL) 31 , 433, 192,077, 444\n",
      "Netherlands 32 , 436, 18,092,525, 41,543\n",
      "Haiti 33 , 423, 11,724,764, 27,750\n",
      "Israel 34 , 416, 9,174,520, 22,072\n",
      "Réunion (France) 35 , 391, 981,796, 2,510\n",
      "Philippines 36 , 391, 117,337,368, 300,000\n",
      "Belgium 37 , 383, 11,686,140, 30,528\n",
      "Comoros 38 , 381, 852,075, 2,235\n",
      "Grenada 39 , 366, 126,184, 345\n",
      "Puerto Rico (US) 40 , 361, 3,205,691, 8,868\n",
      "Martinique (France) 41 , 337, 366,981, 1,090\n",
      "Sri Lanka 42 , 334, 21,893,579, 65,610\n",
      "Japan 43 , 326, 123,294,513, 377,930\n",
      "Guam (US) 44 , 320, 172,952, 541\n",
      "El Salvador 45 , 303, 6,364,943, 21,041\n",
      "Pakistan 46 , 302, 240,485,658, 796,095\n",
      "Trinidad and Tobago 47 , 299, 1,534,937, 5,127\n",
      "Vietnam 48 , 298, 98,858,950, 331,340\n",
      "Saint Lucia 49 , 293, 180,251, 616\n",
      "U.S. Virgin Islands (US) 50 , 285, 98,750, 347\n",
      "United Kingdom 51 , 279, 67,596,281, 242,495\n",
      "Saint Vincent and the Grenadines 52 , 267, 103,699, 389\n",
      "Cayman Islands (UK) 53 , 263, 69,310, 264\n",
      "Jamaica 54 , 257, 2,825,544, 10,991\n",
      "Luxembourg 55 , 253, 654,768, 2,586\n",
      "Liechtenstein 56 , 247, 39,585, 160\n",
      "Gambia 57 , 246, 2,773,168, 11,295\n",
      "Nigeria 58 , 249, 230,842,743, 923,768\n",
      "Kuwait 59 , 242, 4,310,108, 17,818\n",
      "Guadeloupe (France) 60 , 242, 395,839, 1,639\n",
      "São Tomé and Príncipe 61 , 241, 231,856, 964\n",
      "Seychelles 62 , 236, 107,660, 457\n",
      "Qatar 63 , 233, 2,716,391, 11,637\n",
      "Germany 64 , 233, 83,294,633, 357,581\n",
      "Dominican Republic 65 , 233, 11,332,973, 48,671\n",
      "Marshall Islands 66 , 232, 41,996, 181\n",
      "Malawi 67 , 221, 20,931,751, 94,552\n",
      "American Samoa (US) 68 , 221, 43,915, 199\n",
      "North Korea 69 , 217, 26,160,822, 120,538\n",
      "Antigua and Barbuda 70 , 213, 94,298, 442\n",
      "Switzerland 71 , 213, 8,796,669, 41,291\n",
      "Nepal 72 , 210, 30,896,590, 147,181\n",
      "British Virgin Islands (UK) 73 , 209, 31,538, 151\n",
      "Uganda 74 , 201, 48,582,334, 241,550\n",
      "Italy 75 , 195, 58,870,763, 302,068\n",
      "Kiribati 76 , 184, 133,515, 726\n",
      "Saint Kitts and Nevis 77 , 183, 47,755, 261\n",
      "Anguilla (UK) 78 , 175, 15,900, 91\n",
      "Andorra 79 , 171, 80,088, 468\n",
      "Guatemala 80 , 166, 18,092,026, 108,889\n",
      "Micronesia 81 , 164, 115,224, 702\n",
      "Togo 82 , 159, 9,053,799, 56,785\n",
      "Tokelau (New Zealand) 83 , 158, 1,893, 12\n",
      "Kosovo 84 , 153, 1,663,595, 10,905\n",
      "China 85 , 149, 1,425,671,352, 9,600,000\n",
      "Cape Verde 86 , 148, 598,682, 4,033\n",
      "Isle of Man 87 , 148, 84,710, 572\n",
      "Indonesia 88 , 145, 277,534,123, 1,910,931\n",
      "Tonga 89 , 144, 107,773, 747\n",
      "Ghana 90 , 143, 34,121,985, 238,537\n",
      "Thailand 91 , 140, 71,801,279, 513,140\n",
      "Denmark 92 , 138, 5,910,913, 42,947\n",
      "Cyprus 93 , 136, 1,260,138, 9,251\n",
      "United Arab Emirates 94 , 134, 9,516,871, 71,024\n",
      "Transnistria 95 , 134, 468,980, 3,510\n",
      "Czech Republic 96 , 133, 10,495,295, 78,871\n",
      "Jordan 97 , 127, 11,337,053, 89,318\n",
      "Syria 98 , 125, 23,227,014, 185,180\n",
      "Sierra Leone 99 , 122, 8,791,092, 72,300\n",
      "Poland 100 , 120, 37,691,000, 313,931\n",
      "Azerbaijan 101 , 120, 10,412,652, 86,600\n",
      "Benin 102 , 119, 13,712,828, 114,763\n",
      "Slovakia 103 , 110, 5,422,194, 49,035\n",
      "France (metropolitan) 104 , 117, 64,756,584, 551,500\n",
      "Ethiopia 105 , 115, 126,527,060, 1,104,300\n",
      "Northern Cyprus 106 , 114, 382,836, 3,355\n",
      "Egypt 107 , 112, 112,716,599, 1,002,000\n",
      "Portugal 108 , 111, 10,247,605, 92,225\n",
      "Turkey 109 , 110, 85,816,199, 783,562\n",
      "Hungary 110 , 109, 10,156,239, 93,025\n",
      "Northern Mariana Islands (US) 111 , 109, 49,796, 457\n",
      "Austria 112 , 107, 8,958,961, 83,878\n",
      "Iraq 113 , 105, 45,504,560, 435,052\n",
      "Slovenia 114 , 105, 2,119,675, 20,273\n",
      "Malaysia 115 , 104, 34,308,525, 330,621\n",
      "Costa Rica 116 , 102, 5,212,173, 51,100\n",
      "Cuba 117 , 102, 11,194,449, 109,884\n",
      "Moldova 118 , 102, 3,435,931, 33,847\n",
      "France (metropolitan & overseas) 119 , 101, 68,000,000, 672,000\n",
      "Albania 120 , 99, 2,832,439, 28,748\n",
      "Dominica 121 , 97, 73,040, 750\n",
      "Spain 122 , 97, 48,797,875, 505,994\n",
      "Honduras 123 , 94, 10,593,798, 112,492\n",
      "Cambodia 124 , 94, 16,944,826, 181,039\n",
      "Armenia 125 , 93, 2,777,971, 29,743\n",
      "Kenya 126 , 93, 55,100,587, 591,958\n",
      "East Timor 127 , 91, 1,360,596, 14,919\n",
      "Senegal 128 , 90, 17,763,163, 196,712\n",
      "Ivory Coast 129 , 90, 28,873,034, 322,462\n",
      "Burkina Faso 130 , 86, 23,251,485, 270,764\n",
      "French Polynesia (France) 131 , 84, 308,872, 3,687\n",
      "Romania 132 , 83, 19,892,812, 238,398\n",
      "Caribbean Netherlands (Netherlands) 133 , 83, 27,148, 328\n",
      "North Macedonia 134 , 81, 2,085,679, 25,713\n",
      "Wallis and Futuna (France) 135 , 81, 11,502, 142\n",
      "Serbia 136 , 81, 7,149,077, 88,444\n",
      "Myanmar 137 , 81, 54,577,997, 676,577\n",
      "Samoa 138 , 79, 225,681, 2,842\n",
      "Brunei 139 , 78, 452,524, 5,765\n",
      "Greece 140 , 78, 10,341,277, 131,957\n",
      "Uzbekistan 141 , 78, 35,163,944, 448,969\n",
      "Lesotho 142 , 77, 2,330,318, 30,355\n",
      "Tunisia 143 , 76, 12,458,223, 163,610\n",
      "Ireland 144 , 72, 5,056,935, 69,825\n",
      "Cook Islands 145 , 72, 17,044, 236\n",
      "Tajikistan 146 , 72, 10,143,543, 141,400\n",
      "Tanzania 147 , 71, 67,438,106, 947,303\n",
      "Croatia 148 , 71, 4,008,617, 56,594\n",
      "Ecuador 149 , 71, 18,190,484, 257,215\n",
      "Eswatini 150 , 70, 1,210,822, 17,363\n",
      "Mexico 151 , 65, 128,455,567, 1,964,375\n",
      "Yemen 152 , 65, 34,449,825, 527,968\n",
      "Afghanistan 153 , 65, 42,239,854, 652,864\n",
      "Bosnia and Herzegovina 154 , 63, 3,210,848, 51,209\n",
      "Akrotiri and Dhekelia (UK) 155 , 62, 15,700, 254\n",
      "Equatorial Guinea 156 , 61, 1,714,672, 28,051\n",
      "Ukraine 157 , 61, 36,744,634, 603,500\n",
      "Bulgaria 158 , 61, 6,687,717, 110,372\n",
      "Cameroon 159 , 60, 28,647,293, 475,650\n",
      "World (excluding Antarctica) 160 , 60, 8,045,311,448, 134,740,000\n",
      "Guinea-Bissau 161 , 60, 2,150,842, 36,125\n",
      "Panama 162 , 59, 4,468,087, 75,320\n",
      "Guinea 163 , 58, 14,190,612, 245,836\n",
      "Norfolk Island (Australia) 164 , 57, 2,220, 39\n",
      "Iran 165 , 55, 89,172,767, 1,630,848\n",
      "Nicaragua 166 , 54, 7,046,311, 130,373\n",
      "World (all land) 167 , 54, 8,045,311,448, 148,940,000\n",
      "Georgia 168 , 53, 3,728,282, 69,700\n",
      "Morocco 169 , 53, 37,984,655, 710,850\n",
      "Madagascar 170 , 52, 30,325,732, 587,041\n",
      "Fiji 171 , 51, 936,376, 18,272\n",
      "South Africa 172 , 49, 60,414,495, 1,221,037\n",
      "Djibouti 173 , 49, 1,136,455, 23,200\n",
      "Liberia 174 , 49, 5,418,377, 111,369\n",
      "Turks and Caicos Islands (UK) 175 , 49, 46,062, 948\n",
      "Easter Island 176 , 47, 7750, 163.6\n",
      "Belarus 177 , 46, 9,498,238, 207,600\n",
      "Colombia 178 , 46, 52,695,952, 1,141,748\n",
      "Montenegro 179 , 45, 626,485, 13,888\n",
      "DR Congo 180 , 44, 102,262,809, 2,345,410\n",
      "Saint Helena, Ascension and Tristan da Cunha (UK) 181 , 43, 5,314, 123\n",
      "Cocos (Keeling) Islands (Australia) 182 , 43, 602, 14\n",
      "Zimbabwe 183 , 43, 16,665,409, 390,757\n",
      "Montserrat (UK) 184 , 43, 4,387, 103\n",
      "Mozambique 185 , 42, 33,897,354, 799,380\n",
      "Lithuania 186 , 42, 2,718,352, 65,286\n",
      "Palau 187 , 39, 18,058, 459\n",
      "Faroe Islands (Denmark) 188 , 38, 53,270, 1,393\n",
      "United States 189 , 35, 339,996,564, 9,833,517\n",
      "Kyrgyzstan 190 , 34, 6,735,348, 199,949\n",
      "Laos 191 , 32, 7,633,779, 236,800\n",
      "Venezuela 192 , 31, 28,838,499, 929,690\n",
      "Eritrea 193 , 31, 3,748,902, 121,144\n",
      "Bahamas 194 , 30, 412,624, 13,940\n",
      "Angola 195 , 29, 36,684,203, 1,246,700\n",
      "Somaliland 196 , 29, 4,914,100, 167,283\n",
      "Estonia 197 , 29, 1,322,766, 45,399\n",
      "Somalia 198 , 28, 18,143,379, 637,657\n",
      "Latvia 199 , 28, 1,862,700, 64,594\n",
      "Abkhazia 200 , 28, 245,424, 8,662\n",
      "Vanuatu 201 , 27, 334,506, 12,189\n",
      "Zambia 202 , 27, 20,569,738, 752,612\n",
      "Sudan 203 , 27, 48,109,006, 1,765,048\n",
      "Peru 204 , 27, 34,352,719, 1,285,216\n",
      "Chile 205 , 26, 19,629,590, 756,102\n",
      "Solomon Islands 206 , 26, 740,425, 28,896\n",
      "Brazil 207 , 25, 216,422,446, 8,510,346\n",
      "Sweden 208 , 24, 10,612,086, 438,574\n",
      "Saint Pierre and Miquelon (France) 209 , 24, 5,840, 242\n",
      "Papua New Guinea 210 , 22, 10,329,931, 462,840\n",
      "Niger 211 , 21, 27,202,843, 1,267,000\n",
      "Bhutan 212 , 21, 787,425, 38,394\n",
      "Uruguay 213 , 20, 3,423,109, 173,626\n",
      "New Zealand 214 , 20, 5,228,100, 268,107\n",
      "Algeria 215 , 19, 45,606,481, 2,381,741\n",
      "Åland (Finland) 216 , 19, 30,237, 1,583\n",
      "Mali 217 , 19, 23,293,699, 1,240,192\n",
      "Belize 218 , 18, 410,825, 22,965\n",
      "Congo 219 , 18, 6,106,869, 342,000\n",
      "South Sudan 220 , 17, 11,088,796, 658,841\n",
      "Saudi Arabia 221 , 17, 36,947,025, 2,206,714\n",
      "Finland 222 , 16, 5,545,475, 336,884\n",
      "Argentina 223 , 16, 45,773,884, 2,796,427\n",
      "South Ossetia 224 , 16, 57,095, 3,632\n",
      "New Caledonia (France) 225 , 15, 292,991, 19,100\n",
      "Paraguay 226 , 15, 6,109,644, 406,752\n",
      "Oman 227 , 15, 4,644,384, 309,980\n",
      "Chad 228 , 14, 18,278,568, 1,284,000\n",
      "Norway 229 , 14, 5,474,360, 385,207\n",
      "Turkmenistan 230 , 13, 6,516,100, 488,100\n",
      "Christmas Island (Australia) 231 , 12, 1,692, 136\n",
      "Bolivia 232 , 11, 12,388,571, 1,098,581\n",
      "Central African Republic 233 , 9, 5,742,316, 622,984\n",
      "Gabon 234 , 9, 2,436,567, 267,668\n",
      "Russia 235 , 8, 144,444,359, 17,098,246\n",
      "Niue 236 , 7, 1,935, 260\n",
      "Kazakhstan 237 , 7, 19,606,634, 2,724,902\n",
      "Mauritania 238 , 5, 4,862,989, 1,030,700\n",
      "Botswana 239 , 5, 2,675,353, 582,000\n",
      "Libya 240 , 4, 6,888,388, 1,676,198\n",
      "Canada 241 , 4, 41,012,563, 9,984,670\n",
      "Suriname 242 , 4, 623,237, 163,820\n",
      "Guyana 243 , 4, 813,834, 214,969\n",
      "French Guiana (France) 244 , 4, 312,155, 83,534\n",
      "Iceland 245 , 4, 375,319, 103,000\n",
      "Australia 246 , 3, 26,439,112, 7,692,024\n",
      "Namibia 247 , 3, 2,604,172, 825,229\n",
      "Western Sahara (disputed) 248 , 2, 587,259, 266,000\n",
      "Mongolia 249 , 2, 3,447,157, 1,564,116\n",
      "Pitcairn Islands (UK) 250 , 0.9, 40, 47\n",
      "Falkland Islands (UK) 251 , 0.3, 3,791, 12,173\n",
      "Greenland (Denmark) 252 , 0.03, 56,643, 2,166,086\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population_density'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "table = soup.find('table', class_='wikitable')\n",
    "\n",
    "countries = []\n",
    "popk = []\n",
    "pop= []\n",
    "areak = []\n",
    "for row in table.find_all('tr'):\n",
    "    columns = row.find_all('td')\n",
    "    if columns:\n",
    "        country_name = columns[0].text.strip()\n",
    "        pop_km_2 = columns[1].text.strip()\n",
    "        population = columns[3].text.strip()\n",
    "        area_km_2 = columns[4].text.strip()\n",
    "        countries.append(country_name)\n",
    "        popk.append(pop_km_2)\n",
    "        pop.append(population)\n",
    "        areak.append(area_km_2)\n",
    "\n",
    "for i in range(len(countries)):\n",
    "    print(countries[i],i+1,\", \"+popk[i]+\", \"+pop[i]+\", \"+areak[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c15cda",
   "metadata": {},
   "source": [
    "- Save the information obtained in a Python dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca6a04",
   "metadata": {},
   "source": [
    "In this case we just create a dicctionary and create a for loop that for each country on the list it also saves the information of the counter (for the rank because they are in order) and then the information of each list\n",
    "\n",
    "Sources: <br>\n",
    "https://es.stackoverflow.com/questions/151848/agregar-valores-de-una-lista-a-un-diccionario-python <br>\n",
    "https://es.quora.com/C%C3%B3mo-a%C3%B1ado-una-lista-a-un-diccionario-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd64b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = {}\n",
    "for i in range(len(countries)):\n",
    "    country_data[countries[i]] = {\n",
    "        'Range':i+1,\n",
    "        'Density (km²)': popk[i],\n",
    "        'Population': pop[i],\n",
    "        'Area (km²)': areak[i]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcdbb38",
   "metadata": {},
   "source": [
    "- Using the previously saved Python dictionary, ask the user for a country, display the \n",
    "corresponding information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e7f95",
   "metadata": {},
   "source": [
    "Here, we wait for an input of the user, after getting this we check if the country exists, if it does it just returns the information from the dictionary, if it doesn't it prints an error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755011f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of a country: lebanon\n",
      "Information for: Lebanon:\n",
      "Range: 26\n",
      "Population Density (km²): 512\n",
      "Population: 5,353,930\n",
      "Area (km²): 10,452\n"
     ]
    }
   ],
   "source": [
    "country_input = input(\"Enter the name of a country: \").title() # we also used the title() function here to always get a country starting with a capital letter at first\n",
    "\n",
    "if country_input in country_data:\n",
    "    country_info = country_data[country_input]\n",
    "    print(f\"Information for: {country_input}:\")\n",
    "    print(f\"Range: {country_info['Range']}\")\n",
    "    print(f\"Population Density (km²): {country_info['Density (km²)']}\")\n",
    "    print(f\"Population: {country_info['Population']}\")\n",
    "    print(f\"Area (km²): {country_info['Area (km²)']}\")\n",
    "else:\n",
    "    print(f\"No information found for {country_input}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5298c4",
   "metadata": {},
   "source": [
    "# Exercice 6 * - API Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f15e1",
   "metadata": {},
   "source": [
    "- Write a Python program that will make available a Web API allowing elementary calculations on \n",
    "integers.\n",
    "\n",
    "The APIs are accessible by GET and in the form: \n",
    "- /add/{integer1}/{integer2}: add integer1 and integer2\n",
    "- /sub/{integer1}/{integer2}: perform the subtraction of integer1 and integer2\n",
    "- /mul/{integer1}/{integer2}: carry out the multiplication of integer1 and integer2\n",
    "- /div/{integer1}/{integer2}: perform the integer division of integer1 by integer2\n",
    "- /mod/{integer1}/{integer2}: perform the remainder of the integer division of integer1\n",
    "by integer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ca8c8",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7f891b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T07:03:46.583700Z",
     "start_time": "2024-10-11T07:03:36.825290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:8080\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [11/Oct/2024 09:03:41] \"GET /mod/42/8 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='localhost', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d4796",
   "metadata": {},
   "source": [
    "http://localhost:8080/mul/6/7\n",
    "\n",
    "http://localhost:8080/div/42/8\n",
    "\n",
    "http://localhost:8080/mod/42/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582cbf8",
   "metadata": {},
   "source": [
    "- Write a Python program that will test the web API made available through the requests\n",
    "library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804dfee",
   "metadata": {},
   "source": [
    "Answer"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
